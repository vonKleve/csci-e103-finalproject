{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2613aba-b66a-448f-836b-5ac1d2e88c62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, BooleanType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71bf8104-feaf-490d-aaea-496abd169f30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "VOLUME_ROOT_PATH = \"/Volumes/cscie103_catalog/final_project/data\"\n",
    "# place where raw csvs land after download\n",
    "VOLUME_TARGET_DIR = f\"{VOLUME_ROOT_PATH}/raw\"\n",
    "\n",
    "CATALOG_NAME = \"cscie103_catalog\"\n",
    "SCHEMA_NAME = \"final_project\"\n",
    "spark.sql(f\"USE {CATALOG_NAME}.{SCHEMA_NAME}\")\n",
    "\n",
    "class DataframeNames:\n",
    "    HOLIDAYS = \"holidays\"\n",
    "    OIL = \"oil\"\n",
    "    STORES = \"stores\"\n",
    "    TEST = \"test\"\n",
    "    TRAIN = \"train\"\n",
    "    TRANSACTIONS = \"transactions\"\n",
    "    TRAINING = \"training\"\n",
    "\n",
    "    ALL = [ HOLIDAYS, OIL, STORES, TEST, TRAIN, TRANSACTIONS, TRAINING ]\n",
    "\n",
    "class DataTier:\n",
    "    BRONZE = \"bronze\"\n",
    "    SILVER = \"silver\"\n",
    "    GOLD = \"gold\"\n",
    "\n",
    "    def getBronzeName(tablename):\n",
    "        return DataTier.BRONZE + \"_\" + tablename\n",
    "\n",
    "    def getSilverName(tablename):\n",
    "        return DataTier.SILVER + \"_\" + tablename\n",
    "    \n",
    "    def getGoldName(tablename):\n",
    "        return DataTier.GOLD + \"_\" + tablename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05d8d6dc-09fe-4298-8720-165c84d5a384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ensure all volumes exist\n",
    "for volume in [VOLUME_TARGET_DIR]:\n",
    "  dbutils.fs.mkdirs(volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "741546d6-a1fd-41de-b681-bd579238b3e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filenames = {\n",
    "    'holidays': 'holidays_events.csv',\n",
    "    'oil': 'oil.csv',\n",
    "    'stores': 'stores.csv',\n",
    "    'test': 'test.csv',\n",
    "    'train': 'train.csv',\n",
    "    'transactions': 'transactions.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f0fda9-54f5-4baa-bf60-37399a5be6d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extractLoad(csv_filename, bronze_tablename, schema, tbl_ddl_cb):\n",
    "    \"\"\"Extracts and loads a csv file into a bronze table.\"\"\"\n",
    "    print(f\"Loading {csv_filename}...\")\n",
    "    df = spark.read \\\n",
    "      .schema(schema) \\\n",
    "      .csv(f\"{VOLUME_TARGET_DIR}/{csv_filename}\", header=True)\n",
    "    print(f\"Read {csv_filename} with {df.count()} rows.\")\n",
    "\n",
    "    print(f\"Creating bronze table {bronze_tablename}...\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {bronze_tablename}\")\n",
    "    tbl_ddl_cb()\n",
    "    \n",
    "    print(f\"Writing {csv_filename} to bronze table {bronze_tablename}...\")\n",
    "    df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(bronze_tablename)\n",
    "    print(f\"Finished.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a454ecf4-4931-4085-8050-7b0577c4da51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec39eb40-e71d-427b-8359-6cb266f9366d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_filename_stores = filenames.get(DataframeNames.STORES)\n",
    "bronze_tablename_stores = DataTier.getBronzeName(DataframeNames.STORES)\n",
    "stores_schema = StructType([\n",
    "  StructField(\"store_nbr\", IntegerType(), False),\n",
    "  StructField(\"city\", StringType(), True),\n",
    "  StructField(\"state\", StringType(), True),\n",
    "  StructField(\"type\", StringType(), True),\n",
    "  StructField(\"cluster\", IntegerType(), True)\n",
    "])\n",
    "def stores_ddl_cb():\n",
    "  spark.sql(f\"DROP TABLE IF EXISTS {bronze_tablename_stores}\")\n",
    "  spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {bronze_tablename_stores} (\n",
    "      store_nbr INTEGER NOT NULL,  -- Must be NOT NULL for a Primary Key\n",
    "      city STRING,\n",
    "      state STRING,\n",
    "      type STRING,\n",
    "      cluster INTEGER,\n",
    "      \n",
    "      CONSTRAINT pk_store_nbr_v2 -- v2 because Databricks, cannot figure out, without it - fails !!!\n",
    "      PRIMARY KEY (store_nbr)\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Unity Catalog Managed Delta table storing store location and type information.';\n",
    "  \"\"\")\n",
    "\n",
    "extractLoad(csv_filename_stores, bronze_tablename_stores, stores_schema, stores_ddl_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9932cc4f-371a-43db-9676-607b42ad81ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6ccb988-ee93-4b02-98ab-29a2e11c4120",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_filename_transactions = filenames.get(DataframeNames.TRANSACTIONS)\n",
    "bronze_tablename_transactions = DataTier.getBronzeName(DataframeNames.TRANSACTIONS)\n",
    "transactions_schema = StructType([\n",
    "    StructField(\"date\", DateType(), False),\n",
    "    StructField(\"store_nbr\", IntegerType(), False),\n",
    "    StructField(\"transactions\", IntegerType(), True)\n",
    "])\n",
    "def transactions_ddl_cb():\n",
    "  spark.sql(f\"DROP TABLE IF EXISTS {bronze_tablename_transactions}\")\n",
    "  spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {bronze_tablename_transactions} (\n",
    "      date DATE NOT NULL,\n",
    "      store_nbr INTEGER NOT NULL,\n",
    "      transactions INTEGER,\n",
    "      \n",
    "      CONSTRAINT pk_transaction_id \n",
    "      PRIMARY KEY (date, store_nbr),\n",
    "      \n",
    "      CONSTRAINT fk_txn_store_nbr \n",
    "      FOREIGN KEY (store_nbr)\n",
    "      REFERENCES {CATALOG_NAME}.{SCHEMA_NAME}.{bronze_tablename_stores} (store_nbr)\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Unity Catalog Managed Delta table storing daily transaction counts per store with PK and FK constraints.';\n",
    "  \"\"\")\n",
    "\n",
    "extractLoad(csv_filename_transactions, bronze_tablename_transactions, transactions_schema, transactions_ddl_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fc500c5-aa1d-4ce4-9877-82d8165fad5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Oil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50891b2c-f06e-4b26-bac9-a324cab567f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_filename_oil = filenames.get(DataframeNames.OIL)\n",
    "bronze_tablename_oil = DataTier.getBronzeName(DataframeNames.OIL)\n",
    "oil_schema = StructType([\n",
    "    StructField(\"date\", DateType(), False),\n",
    "    StructField(\"dcoilwtico\", DoubleType(), True)\n",
    "])\n",
    "def oil_ddl_cb():\n",
    "  spark.sql(f\"DROP TABLE IF EXISTS {bronze_tablename_oil}\")\n",
    "  spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {bronze_tablename_oil} (\n",
    "    date DATE NOT NULL,\n",
    "    dcoilwtico DOUBLE,\n",
    "    \n",
    "    -- Primary Key: Ensures a unique price per date.\n",
    "    CONSTRAINT pk_oil_date \n",
    "      PRIMARY KEY (date)\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Unity Catalog Managed Delta table storing daily WTI Crude Oil prices with PK constraint.';\n",
    "  \"\"\")\n",
    "\n",
    "extractLoad(csv_filename_oil, bronze_tablename_oil, oil_schema, oil_ddl_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75da4824-9b26-4f0e-b0e4-4b86e4cb4b20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a04fd109-bb75-40f7-9454-c7676ab36ed8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_filename_holidays_events = filenames.get(DataframeNames.HOLIDAYS)\n",
    "bronze_tablename_holidays_events = DataTier.getBronzeName(DataframeNames.HOLIDAYS)\n",
    "holidays_schema = StructType([\n",
    "    StructField(\"date\", DateType(), False),      # Date is essential, set to NOT NULL\n",
    "    StructField(\"type\", StringType(), False),    # Holiday type is essential, set to NOT NULL\n",
    "    StructField(\"locale\", StringType(), False),  # Defines the scope (National/Regional), NOT NULL\n",
    "    StructField(\"locale_name\", StringType(), False), # Specific region name, NOT NULL\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"transferred\", BooleanType(), True)\n",
    "])\n",
    "def holidays_ddl_cb():\n",
    "  spark.sql(f\"DROP TABLE IF EXISTS {bronze_tablename_holidays_events}\")\n",
    "  spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {bronze_tablename_holidays_events} (\n",
    "      date DATE NOT NULL,\n",
    "      type STRING NOT NULL,\n",
    "      locale STRING NOT NULL,\n",
    "      locale_name STRING NOT NULL,\n",
    "      description STRING,\n",
    "      transferred BOOLEAN,\n",
    "      \n",
    "      CONSTRAINT pk_holiday_id \n",
    "      PRIMARY KEY (date, type, locale, locale_name)\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Unity Catalog Managed Delta table storing holiday data with Composite PK constraint.';\n",
    "  \"\"\")\n",
    "\n",
    "extractLoad(csv_filename_holidays_events, bronze_tablename_holidays_events, holidays_schema, holidays_ddl_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76f9f317-eb04-4e85-9f6a-f451174411d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f17be9e5-f7c6-4d89-9d9a-bd7074c17d21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_filename_train = filenames.get(DataframeNames.TRAIN)\n",
    "bronze_tablename_train = DataTier.getBronzeName(DataframeNames.TRAIN)\n",
    "train_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),        # Unique identifier, set to NOT NULL\n",
    "    StructField(\"date\", DateType(), False),         # Essential for time series, set to NOT NULL\n",
    "    StructField(\"store_nbr\", IntegerType(), False),  # Links to stores table, set to NOT NULL\n",
    "    StructField(\"family\", StringType(), False),      # Product category, set to NOT NULL\n",
    "    StructField(\"sales\", DoubleType(), True),       # Sales amount (can be 0 or null depending on source)\n",
    "    StructField(\"onpromotion\", IntegerType(), True)  # Number of items on promotion\n",
    "])\n",
    "def train_ddl_cb():\n",
    "  spark.sql(f\"DROP TABLE IF EXISTS {bronze_tablename_train}\")\n",
    "  spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {bronze_tablename_train} (\n",
    "        id INTEGER NOT NULL,\n",
    "        date DATE NOT NULL,\n",
    "        store_nbr INTEGER NOT NULL,\n",
    "        family STRING NOT NULL,\n",
    "        sales DOUBLE,\n",
    "        onpromotion INTEGER,\n",
    "        \n",
    "        CONSTRAINT pk_sales_record \n",
    "        PRIMARY KEY (date, store_nbr, family), \n",
    "        \n",
    "        CONSTRAINT fk_sales_store_nbr \n",
    "        FOREIGN KEY (store_nbr)\n",
    "        REFERENCES {CATALOG_NAME}.{SCHEMA_NAME}.{bronze_tablename_stores} (store_nbr)\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Unity Catalog Managed Delta table storing daily sales training data with Composite PK and FK constraints.';\n",
    "  \"\"\")\n",
    "\n",
    "extractLoad(csv_filename_train, bronze_tablename_train, train_schema, train_ddl_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "936f36d2-942d-47b4-a3dd-4e06cf0675c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf012624-d6ec-4a88-8010-7aaa6f44607f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_filename_test = filenames.get(DataframeNames.TEST)\n",
    "bronze_tablename_test = DataTier.getBronzeName(DataframeNames.TEST)\n",
    "test_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),        # Unique identifier, set to NOT NULL\n",
    "    StructField(\"date\", DateType(), False),         # Essential for time series, set to NOT NULL\n",
    "    StructField(\"store_nbr\", IntegerType(), False),  # Links to stores table, set to NOT NULL\n",
    "    StructField(\"family\", StringType(), False),      # Product category, set to NOT NULL\n",
    "    StructField(\"onpromotion\", IntegerType(), True)  # Number of items on promotion (can be null/missing)\n",
    "])\n",
    "def test_ddl_cb():\n",
    "  spark.sql(f\"DROP TABLE IF EXISTS {bronze_tablename_test}\")\n",
    "  spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {bronze_tablename_test} (\n",
    "        id INTEGER NOT NULL,\n",
    "        date DATE NOT NULL,\n",
    "        store_nbr INTEGER NOT NULL,\n",
    "        family STRING NOT NULL,\n",
    "        onpromotion INTEGER,\n",
    "        \n",
    "        CONSTRAINT pk_test_id \n",
    "        PRIMARY KEY (id), \n",
    "        \n",
    "        CONSTRAINT fk_test_store_nbr \n",
    "        FOREIGN KEY (store_nbr)\n",
    "        REFERENCES {CATALOG_NAME}.{SCHEMA_NAME}.{bronze_tablename_stores} (store_nbr)\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Unity Catalog Managed Delta table storing sales test data with PK and FK constraints.';\n",
    "    \"\"\")\n",
    "\n",
    "extractLoad(csv_filename_test, bronze_tablename_test, test_schema, test_ddl_cb)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4938962754428469,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02-bronze-tier",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
