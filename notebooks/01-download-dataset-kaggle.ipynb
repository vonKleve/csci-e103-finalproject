{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87ef88d2-faa1-41c0-9a41-177dec2356d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook serves as a sandbox to provide exploratory data analysis in preparation for ERD & E2E workflow specifications.\n",
    "It will:\n",
    "1. Create schema & volume if needed.\n",
    "2. Fetch data from [Kaggle competition](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview).\n",
    "3. Create respective tables per csv file.\n",
    "\n",
    "... (wip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd588b5c-dba1-41e5-896f-ca05a842e98a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- creates cscie103_catalog.final_project schema and data volume (if not exist)\n",
    "CREATE SCHEMA IF NOT EXISTS cscie103_catalog.final_project;\n",
    "CREATE VOLUME IF NOT EXISTS cscie103_catalog.final_project.data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c90597be-fc72-4f28-862f-e0d19632c61b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ebde8b8-e1db-4d04-95fa-e292533a6cf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%py\n",
    "# all imports here\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import zipfile\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e7c4c67-c1b9-4f6f-a5ff-3021eecddd6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.workspace import CreateScope\n",
    "\n",
    "w = WorkspaceClient()\n",
    "SCOPE = \"kaggle\"\n",
    "\n",
    "try:\n",
    "    # Create scope (will error if already exists)\n",
    "    w.secrets.create_scope(scope=SCOPE)\n",
    "    print(f\"‚úì Created scope '{SCOPE}'\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(f\"‚Ñπ Scope '{SCOPE}' already exists\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Add secrets\n",
    "# fetch from github secrets\n",
    "_username = dbutils.secrets.get(\n",
    "    scope=\"e-103-finalproject-credentials\",\n",
    "    key=\"kaggle-username\"\n",
    ")\n",
    "_token = dbutils.secrets.get(\n",
    "    scope=\"e-103-finalproject-credentials\",\n",
    "    key=\"kaggle-api-token\"\n",
    ")\n",
    "secrets_to_add = {\n",
    "    \"kaggle-username\": _username,\n",
    "    \"kaggle-api-token\": _token\n",
    "}\n",
    "\n",
    "for key, value in secrets_to_add.items():\n",
    "    w.secrets.put_secret(scope=SCOPE, key=key, string_value=value)\n",
    "    print(f\"‚úì Added secret '{key}'\")\n",
    "\n",
    "# Verify secrets were added\n",
    "print(f\"\\nSecrets in '{SCOPE}':\")\n",
    "for secret in w.secrets.list_secrets(scope=SCOPE):\n",
    "    print(f\"  - {secret.key} (last updated: {secret.last_updated_timestamp})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ff7d0b0-49d9-46a6-9d41-43997b70e216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For READING secrets in notebooks, continue using dbutils\n",
    "username = dbutils.secrets.get(scope=SCOPE, key=\"kaggle-username\")\n",
    "api_key = dbutils.secrets.get(scope=SCOPE, key=\"kaggle-api-token\")\n",
    "\n",
    "print(f\"‚úì Retrieved username (length: {len(username)})\")\n",
    "print(f\"‚úì Retrieved API key (length: {len(api_key)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85a62cf1-2cc6-4afc-b911-2d1928a10e06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Kaggle Data Download using Official Python API\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- STEP 1: Configure paths ----\n",
    "COMPETITION_NAME = \"store-sales-time-series-forecasting\"\n",
    "VOLUME_ROOT_PATH = \"/Volumes/cscie103_catalog/final_project/data\"\n",
    "VOLUME_TARGET_DIR = f\"{VOLUME_ROOT_PATH}/raw\"\n",
    "DOWNLOAD_PATH = VOLUME_TARGET_DIR\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(DOWNLOAD_PATH, exist_ok=True)\n",
    "print(f\"‚úì Target directory: {DOWNLOAD_PATH}\")\n",
    "\n",
    "# ---- STEP 2: Set up Kaggle credentials ----\n",
    "# Retrieve credentials from Databricks secrets\n",
    "try:\n",
    "    kaggle_username = dbutils.secrets.get(scope=\"kaggle\", key=\"kaggle-username\")\n",
    "    kaggle_token = dbutils.secrets.get(scope=\"kaggle\", key=\"kaggle-api-token\")\n",
    "    print(\"‚úì Successfully retrieved Kaggle credentials from secrets\")\n",
    "    print(f\"  Username: {kaggle_username}\")\n",
    "    print(f\"  Token length: {len(kaggle_token)} characters\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error retrieving secrets: {e}\")\n",
    "    print(\"\\nTo set up secrets, run these commands in Databricks CLI:\")\n",
    "    print('  databricks secrets create-scope --scope kaggle')\n",
    "    print('  databricks secrets put --scope kaggle --key username')\n",
    "    print('  databricks secrets put --scope kaggle --key token')\n",
    "    raise\n",
    "\n",
    "# Create kaggle.json configuration file\n",
    "kaggle_dir = os.path.expanduser(\"~/.kaggle\")\n",
    "os.makedirs(kaggle_dir, exist_ok=True)\n",
    "\n",
    "kaggle_config_path = os.path.join(kaggle_dir, \"kaggle.json\")\n",
    "kaggle_config = {\n",
    "    \"username\": kaggle_username,\n",
    "    \"key\": kaggle_token\n",
    "}\n",
    "\n",
    "with open(kaggle_config_path, \"w\") as f:\n",
    "    json.dump(kaggle_config, f)\n",
    "\n",
    "# Set proper permissions (Kaggle requires this)\n",
    "os.chmod(kaggle_config_path, 0o600)\n",
    "print(f\"‚úì Created Kaggle config at: {kaggle_config_path}\")\n",
    "\n",
    "# ---- STEP 3: Install and import Kaggle API ----\n",
    "try:\n",
    "    import kaggle\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    print(f\"‚úì Kaggle package version: {kaggle.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing Kaggle package...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"pip\", \"install\", \"kaggle\", \"-q\"], check=True)\n",
    "    import kaggle\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    print(\"‚úì Kaggle package installed\")\n",
    "\n",
    "# ---- STEP 4: Authenticate and download ----\n",
    "if not os.path.exists(f\"{VOLUME_TARGET_DIR}/train.csv\"):\n",
    "    print(f\"\\nüì• Downloading competition data: {COMPETITION_NAME}\")\n",
    "    print(\"‚ö†Ô∏è  NOTE: If this fails with 403, you must accept competition rules at:\")\n",
    "    print(f\"   https://www.kaggle.com/competitions/{COMPETITION_NAME}/rules\")\n",
    "    print(\"   Click 'I Understand and Accept' button, then re-run this cell\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize and authenticate API\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        print(\"‚úì API authenticated successfully\")\n",
    "        \n",
    "        # Download competition files\n",
    "        print(f\"üìÇ Downloading files to: {DOWNLOAD_PATH}\")\n",
    "        api.competition_download_files(\n",
    "            competition=COMPETITION_NAME,\n",
    "            path=DOWNLOAD_PATH,\n",
    "            quiet=False\n",
    "        )\n",
    "        print(\"‚úì Download complete\")\n",
    "        \n",
    "        # Unzip the downloaded file\n",
    "        import zipfile\n",
    "        zip_file = f\"{DOWNLOAD_PATH}/{COMPETITION_NAME}.zip\"\n",
    "        \n",
    "        if os.path.exists(zip_file):\n",
    "            print(f\"\\nüìÇ Extracting files from: {zip_file}\")\n",
    "            with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
    "                file_list = zip_ref.namelist()\n",
    "                zip_ref.extractall(DOWNLOAD_PATH)\n",
    "                print(f\"‚úì Extracted {len(file_list)} files:\")\n",
    "                for file in file_list:\n",
    "                    file_size = os.path.getsize(f\"{DOWNLOAD_PATH}/{file}\") / (1024*1024)\n",
    "                    print(f\"  ‚Ä¢ {file} ({file_size:.2f} MB)\")\n",
    "            \n",
    "            # Clean up ZIP file\n",
    "            os.remove(zip_file)\n",
    "            print(\"‚úì Cleaned up ZIP file\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Warning: ZIP file not found at {zip_file}\")\n",
    "            # List downloaded files\n",
    "            print(\"\\nDownloaded files:\")\n",
    "            for file in os.listdir(DOWNLOAD_PATH):\n",
    "                print(f\"  ‚Ä¢ {file}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"‚úó Error during download: {error_msg}\")\n",
    "        \n",
    "        if \"403\" in error_msg or \"Forbidden\" in error_msg:\n",
    "            print(\"\\n‚ùå ACCESS FORBIDDEN (403)\")\n",
    "            print(\"You must accept the competition rules first:\")\n",
    "            print(f\"  1. Visit: https://www.kaggle.com/competitions/{COMPETITION_NAME}/rules\")\n",
    "            print(\"  2. Scroll down and click 'I Understand and Accept' button\")\n",
    "            print(\"  3. Re-run this cell\")\n",
    "        elif \"401\" in error_msg or \"Unauthorized\" in error_msg:\n",
    "            print(\"\\n‚ùå AUTHENTICATION FAILED (401)\")\n",
    "            print(\"Your credentials are incorrect or expired:\")\n",
    "            print(\"  1. Go to: https://www.kaggle.com/settings/account\")\n",
    "            print(\"  2. Scroll to 'API' section\")\n",
    "            print(\"  3. Click 'Create New Token' (downloads kaggle.json)\")\n",
    "            print(\"  4. Update your Databricks secrets with the new username and key\")\n",
    "        \n",
    "        raise\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Skipped downloading because train.csv already exists.\")\n",
    "\n",
    "# ---- STEP 5: List all downloaded files ----\n",
    "print(\"\\nüìÅ Files in target directory:\")\n",
    "files = os.listdir(VOLUME_TARGET_DIR)\n",
    "for file in sorted(files):\n",
    "    file_path = os.path.join(VOLUME_TARGET_DIR, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        file_size = os.path.getsize(file_path) / (1024*1024)\n",
    "        print(f\"  ‚Ä¢ {file} ({file_size:.2f} MB)\")\n",
    "\n",
    "# ---- STEP 6: Verify data ----\n",
    "print(\"\\nüîç Verifying train.csv...\")\n",
    "file_path = f\"{VOLUME_TARGET_DIR}/train.csv\"\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    try:\n",
    "        df = (\n",
    "            spark.read\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .csv(file_path)\n",
    "        )\n",
    "        \n",
    "        row_count = df.count()\n",
    "        col_count = len(df.columns)\n",
    "        \n",
    "        print(f\"‚úì Data loaded into Spark DataFrame\")\n",
    "        print(f\"  Rows: {row_count:,}\")\n",
    "        print(f\"  Columns: {col_count}\")\n",
    "        print(f\"\\nüìã Schema:\")\n",
    "        df.printSchema()\n",
    "        \n",
    "        print(f\"\\nüìä Sample data (first 5 rows):\")\n",
    "        df.show(5, truncate=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error reading file from Volume: {e}\")\n",
    "        raise\n",
    "else:\n",
    "    print(f\"‚úó train.csv not found at: {file_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ All done! Data is ready for analysis.\")\n",
    "\n",
    "# Optional: Clean up credentials file for security\n",
    "# os.remove(kaggle_config_path)\n",
    "# print(\"‚úì Cleaned up Kaggle credentials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "741546d6-a1fd-41de-b681-bd579238b3e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filenames = {\n",
    "    'holidays_events': 'holidays_events.csv',\n",
    "    'oil': 'oil.csv',\n",
    "    'sample_submission': 'sample_submission.csv',\n",
    "    'stores': 'stores.csv',\n",
    "    'test': 'test.csv',\n",
    "    'train': 'train.csv',\n",
    "    'transactions': 'transactions.csv'\n",
    "}\n",
    "\n",
    "holidays_events_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('holidays_events')}\", header=True, inferSchema=True)\n",
    "oil_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('oil')}\", header=True, inferSchema=True)\n",
    "stores_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('stores')}\", header=True, inferSchema=True)\n",
    "transactions_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('transactions')}\", header=True, inferSchema=True)\n",
    "train_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('train')}\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b425865-91c6-4860-8402-e76a17df701e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Delete a secret (if needed)\n",
    "w.secrets.delete_secret(scope=SCOPE, key=\"kaggle-username\")\n",
    "w.secrets.delete_secret(scope=SCOPE, key=\"kaggle-api-token\")\n",
    "\n",
    "# 6. Delete entire scope (if needed)\n",
    "# w.secrets.delete_scope(scope=SCOPE)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5282057407318133,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01-download-dataset-kaggle",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
