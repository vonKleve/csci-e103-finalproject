{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fe11333-1344-468b-a838-338ed5b3cfc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MLflow Model Training Pipeline\n",
    "\n",
    "**Objective:** Train a LightGBM model to forecast store sales with MLflow experiment tracking\n",
    "\n",
    "**Pipeline:**\n",
    "1. Load data from Silver layer\n",
    "2. Feature engineering\n",
    "3. Model training with validation\n",
    "4. Experiment tracking & model registration\n",
    "5. Performance visualization\n",
    "6. Generate predictions for Gold layer\n",
    "\n",
    "**Model:** LightGBM Regressor  \n",
    "**Target:** Sales forecasting (log-transformed)  \n",
    "**Metric:** RMSLE (Root Mean Squared Logarithmic Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a17a45de-3be6-46a5-84e1-ce0a4cb5ff30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install -q mlflow lightgbm\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import mlflow\n",
    "import mlflow.lightgbm\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37ab81eb-f3dc-4338-85e4-8da833610273",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MLflow Configuration\n",
    "# Get current username for experiment path\n",
    "current_user = spark.sql(\"SELECT current_user() as user\").collect()[0][\"user\"]\n",
    "EXPERIMENT_NAME = f\"/Users/{current_user}/store_sales_forecast\"\n",
    "MODEL_NAME = \"store_sales_lgbm\"\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "print(f\"✓ MLflow experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"✓ Model name: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ee3c26d-361e-4ae6-af35-b256606fac3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Load Data from Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eabef6c-0795-4191-91c6-84afab03e726",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Silver tables\n",
    "spark.sql(\"USE cscie103_catalog.final_project\")\n",
    "\n",
    "print(\"Loading data from Silver layer...\")\n",
    "train_df = spark.table(\"silver_train\").toPandas()\n",
    "test_df = spark.table(\"silver_test\").toPandas()\n",
    "oil_df = spark.table(\"silver_oil\").toPandas()\n",
    "holidays_df = spark.table(\"silver_holidays\").toPandas()\n",
    "stores_df = spark.table(\"silver_stores\").toPandas()\n",
    "transactions_df = spark.table(\"silver_transactions\").toPandas()\n",
    "\n",
    "print(f\"Train: {len(train_df):,} rows\")\n",
    "print(f\"Test: {len(test_df):,} rows\")\n",
    "print(f\"Oil: {len(oil_df):,} rows\")\n",
    "print(f\"Holidays: {len(holidays_df):,} rows\")\n",
    "print(f\"Stores: {len(stores_df):,} rows\")\n",
    "print(f\"Transactions: {len(transactions_df):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df0ee9ca-5f6b-41ef-8978-2bf0d37f7ff2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da7f2b1-5152-42d3-ba8f-485e0d4c1eb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert date columns to datetime\n",
    "print(\"Converting date columns...\")\n",
    "for df in [train_df, test_df, oil_df, holidays_df, transactions_df]:\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "print(\"Date conversion complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc578e58-85c4-45fc-88fb-9e7c3d7c96c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Temporal features: day of week and weekend indicator\n",
    "print(\"Creating temporal features...\")\n",
    "\n",
    "train_df[\"day_of_week\"] = train_df[\"date\"].dt.dayofweek\n",
    "test_df[\"day_of_week\"] = test_df[\"date\"].dt.dayofweek\n",
    "\n",
    "train_df[\"is_weekend\"] = train_df[\"day_of_week\"].isin([5, 6]).astype(int)\n",
    "test_df[\"is_weekend\"] = test_df[\"day_of_week\"].isin([5, 6]).astype(int)\n",
    "\n",
    "print(\"✓ Temporal features created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b84b18-30c6-4a87-9d12-c01c0b12cbc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Oil price features: forward/backward fill missing values\n",
    "print(\"Processing oil prices...\")\n",
    "\n",
    "# Create complete date range\n",
    "full_date_range = pd.date_range(train_df[\"date\"].min(), test_df[\"date\"].max(), freq=\"D\")\n",
    "\n",
    "# Reindex oil data to fill gaps\n",
    "oil_df = oil_df.set_index(\"date\").reindex(full_date_range)\n",
    "oil_df.index.name = \"date\"\n",
    "\n",
    "# Fill missing values (forward fill, then backward fill, then median)\n",
    "oil_df[\"dcoilwtico\"] = oil_df[\"dcoilwtico\"].ffill().bfill()\n",
    "oil_df[\"dcoilwtico\"] = oil_df[\"dcoilwtico\"].fillna(oil_df[\"dcoilwtico\"].median())\n",
    "\n",
    "# Rename column for clarity\n",
    "oil_df = oil_df.reset_index().rename(columns={\"dcoilwtico\": \"oil_price\"})\n",
    "\n",
    "print(f\"✓ Oil prices processed, missing values filled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a293fc70-67d5-4fb7-8990-7b6f72ab7ef4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Holiday features\n",
    "print(\"Creating holiday features...\")\n",
    "\n",
    "# Create binary holiday indicator (1 if not a work day)\n",
    "holidays_df[\"is_holiday\"] = (holidays_df[\"type\"] != \"Work Day\").astype(int)\n",
    "\n",
    "# Aggregate by date (max in case of multiple holidays per day)\n",
    "holidays_agg = holidays_df.groupby(\"date\")[\"is_holiday\"].max().reset_index()\n",
    "\n",
    "print(f\"✓ Holiday features created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d787187-3385-40af-b861-6802d1aaa40f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Salary day feature (15th and 30th of month)\n",
    "print(\"Creating salary day indicator...\")\n",
    "\n",
    "def is_salary_day(date):\n",
    "    \"\"\"Returns 1 if date is 15th or 30th (typical salary days in Ecuador)\"\"\"\n",
    "    return int(date.day in [15, 30])\n",
    "\n",
    "train_df[\"is_salary_day\"] = train_df[\"date\"].map(is_salary_day)\n",
    "test_df[\"is_salary_day\"] = test_df[\"date\"].map(is_salary_day)\n",
    "\n",
    "print(\"✓ Salary day indicator created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "868f027e-873e-4ec9-9774-21e91a0d1bb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Earthquake impact feature (April 16, 2016 earthquake in Ecuador)\n",
    "print(\"Creating earthquake impact feature...\")\n",
    "\n",
    "EARTHQUAKE_DATE = pd.to_datetime(\"2016-04-16\")\n",
    "EARTHQUAKE_WINDOW_DAYS = 15\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    df[\"earthquake_impact\"] = (\n",
    "        (df[\"date\"] >= EARTHQUAKE_DATE - pd.Timedelta(EARTHQUAKE_WINDOW_DAYS, \"D\")) & \n",
    "        (df[\"date\"] <= EARTHQUAKE_DATE + pd.Timedelta(EARTHQUAKE_WINDOW_DAYS, \"D\"))\n",
    "    ).astype(int)\n",
    "\n",
    "print(f\"✓ Earthquake impact window: ±{EARTHQUAKE_WINDOW_DAYS} days from {EARTHQUAKE_DATE.date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "792a7012-226d-4d95-94b2-95161f778e3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Merge external features\n",
    "print(\"Merging external features...\")\n",
    "\n",
    "train_df = train_df.merge(oil_df, on=\"date\", how=\"left\")\n",
    "test_df = test_df.merge(oil_df, on=\"date\", how=\"left\")\n",
    "\n",
    "train_df = train_df.merge(holidays_agg, on=\"date\", how=\"left\")\n",
    "test_df = test_df.merge(holidays_agg, on=\"date\", how=\"left\")\n",
    "\n",
    "# Fill missing holidays with 0\n",
    "train_df[\"is_holiday\"] = train_df[\"is_holiday\"].fillna(0).astype(int)\n",
    "test_df[\"is_holiday\"] = test_df[\"is_holiday\"].fillna(0).astype(int)\n",
    "\n",
    "print(\"✓ External features merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8eddf77-33c2-4888-81f1-2adb1d681e9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Store features: encode categorical variables\n",
    "print(\"Encoding store features...\")\n",
    "\n",
    "categorical_cols = [\"city\", \"state\", \"type\", \"cluster\"]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    stores_df[col] = stores_df[col].astype(str)\n",
    "    # Create mapping\n",
    "    unique_values = stores_df[col].unique()\n",
    "    encoding_map = {value: idx for idx, value in enumerate(unique_values)}\n",
    "    stores_df[col] = stores_df[col].map(encoding_map).astype(int)\n",
    "    print(f\"  • {col}: {len(unique_values)} unique values encoded\")\n",
    "\n",
    "# Merge store features\n",
    "train_df = train_df.merge(stores_df, on=\"store_nbr\", how=\"left\")\n",
    "test_df = test_df.merge(stores_df, on=\"store_nbr\", how=\"left\")\n",
    "\n",
    "print(\"✓ Store features encoded and merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa15f2be-b9d4-4e36-acfa-8a3b54fc04f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transaction features\n",
    "print(\"Merging transaction data...\")\n",
    "\n",
    "train_df = train_df.merge(transactions_df, on=[\"store_nbr\", \"date\"], how=\"left\")\n",
    "test_df = test_df.merge(transactions_df, on=[\"store_nbr\", \"date\"], how=\"left\")\n",
    "\n",
    "# Fill missing transactions with 0\n",
    "train_df[\"transactions\"] = train_df[\"transactions\"].fillna(0)\n",
    "test_df[\"transactions\"] = test_df[\"transactions\"].fillna(0)\n",
    "\n",
    "print(\"✓ Transaction data merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "759fa010-7ac6-4838-8c5e-962d9b30aee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Zero sales streak feature\n",
    "print(\"Calculating zero sales streak...\")\n",
    "\n",
    "train_df = train_df.sort_values([\"store_nbr\", \"family\", \"date\"])\n",
    "train_df[\"is_zero_sales\"] = (train_df[\"sales\"] == 0).astype(int)\n",
    "\n",
    "# Calculate consecutive zero sales days\n",
    "zero_streaks = []\n",
    "streak_count = 0\n",
    "prev_store = None\n",
    "prev_family = None\n",
    "\n",
    "for _, row in train_df[[\"store_nbr\", \"family\", \"is_zero_sales\"]].iterrows():\n",
    "    # Reset streak if store/family changes\n",
    "    if row[\"store_nbr\"] != prev_store or row[\"family\"] != prev_family:\n",
    "        streak_count = 0\n",
    "    \n",
    "    # Update streak\n",
    "    if row[\"is_zero_sales\"] == 1:\n",
    "        streak_count += 1\n",
    "    else:\n",
    "        streak_count = 0\n",
    "    \n",
    "    zero_streaks.append(streak_count)\n",
    "    prev_store = row[\"store_nbr\"]\n",
    "    prev_family = row[\"family\"]\n",
    "\n",
    "train_df[\"zero_sales_streak\"] = zero_streaks\n",
    "\n",
    "# Store closed indicator (14+ days of zero sales)\n",
    "train_df[\"store_closed\"] = (train_df[\"zero_sales_streak\"] >= 14).astype(int)\n",
    "test_df[\"store_closed\"] = 0  # Assume stores open for test period\n",
    "\n",
    "print(f\"✓ Zero sales streaks calculated\")\n",
    "print(f\"  • Max streak: {train_df['zero_sales_streak'].max()} days\")\n",
    "print(f\"  • Closed store instances: {train_df['store_closed'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7ebdc01-312d-405f-b8b6-9dc1e45f460c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Family (product category) encoding\n",
    "print(\"Encoding product families...\")\n",
    "\n",
    "train_df[\"family\"] = train_df[\"family\"].astype(str)\n",
    "test_df[\"family\"] = test_df[\"family\"].astype(str)\n",
    "\n",
    "# Create unified encoding for train and test\n",
    "all_families = pd.concat([train_df[\"family\"], test_df[\"family\"]]).unique()\n",
    "family_encoding = {family: idx for idx, family in enumerate(all_families)}\n",
    "\n",
    "train_df[\"family_encoded\"] = train_df[\"family\"].map(family_encoding)\n",
    "test_df[\"family_encoded\"] = test_df[\"family\"].map(family_encoding)\n",
    "\n",
    "print(f\"✓ {len(all_families)} product families encoded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15682810-19ad-4d21-94d2-61289b175ece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "791ee979-143b-42d9-b768-ac58f83e38da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "FEATURE_COLS = [\n",
    "    \"family_encoded\", \"store_nbr\", \"city\", \"state\", \"type\", \"cluster\",\n",
    "    \"oil_price\", \"is_holiday\", \"is_salary_day\", \"earthquake_impact\",\n",
    "    \"transactions\", \"store_closed\", \"day_of_week\", \"is_weekend\"\n",
    "]\n",
    "\n",
    "print(f\"Selected features ({len(FEATURE_COLS)}):\")\n",
    "for i, feature in enumerate(FEATURE_COLS, 1):\n",
    "    print(f\"  {i:2d}. {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b112053-cbf4-42dc-ac7c-b41f467f3729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare feature matrices and target\n",
    "X_train_full = train_df[FEATURE_COLS]\n",
    "y_train_full = train_df[\"sales\"].astype(float)\n",
    "X_test = test_df[FEATURE_COLS]\n",
    "\n",
    "print(f\"✓ Features prepared\")\n",
    "print(f\"  • Training samples: {len(X_train_full):,}\")\n",
    "print(f\"  • Test samples: {len(X_test):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d5d5c5d-caf6-4d8e-a0ae-21c32d4f890d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train/validation split: last 28 days for validation\n",
    "VALIDATION_DAYS = 28\n",
    "\n",
    "validation_cutoff_date = train_df[\"date\"].max() - pd.Timedelta(VALIDATION_DAYS, \"D\")\n",
    "\n",
    "train_mask = train_df[\"date\"] <= validation_cutoff_date\n",
    "val_mask = train_df[\"date\"] > validation_cutoff_date\n",
    "\n",
    "X_train = X_train_full[train_mask]\n",
    "y_train = y_train_full[train_mask]\n",
    "X_val = X_train_full[val_mask]\n",
    "y_val = y_train_full[val_mask]\n",
    "\n",
    "# Log transform target (sales are always positive)\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_val_log = np.log1p(y_val)\n",
    "\n",
    "print(f\"✓ Train/validation split:\")\n",
    "print(f\"  • Training: {len(X_train):,} samples (up to {validation_cutoff_date.date()})\")\n",
    "print(f\"  • Validation: {len(X_val):,} samples (last {VALIDATION_DAYS} days)\")\n",
    "print(f\"  • Log transformation applied to target variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f57e144e-81cd-4570-96fb-bafde0d2f9c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Model Training with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b82760c3-323e-4e6f-bd8b-f117dd13815f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enable MLflow autologging for LightGBM\n",
    "mlflow.lightgbm.autolog()\n",
    "\n",
    "print(\"Starting model training with MLflow tracking...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"lgbm_baseline\") as run:\n",
    "    # Model hyperparameters\n",
    "    params = {\n",
    "        \"n_estimators\": 1000,\n",
    "        \"learning_rate\": 0.03,\n",
    "        \"num_leaves\": 64,\n",
    "        \"min_data_in_leaf\": 50,\n",
    "        \"feature_fraction\": 0.8,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"bagging_freq\": 3,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "    \n",
    "    print(\"Hyperparameters:\")\n",
    "    for key, value in params.items():\n",
    "        print(f\"  • {key}: {value}\")\n",
    "    \n",
    "    # Initialize and train model\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train_log,\n",
    "        eval_set=[(X_train, y_train_log), (X_val, y_val_log)],\n",
    "        eval_metric=\"rmse\"\n",
    "    )\n",
    "    \n",
    "    # Generate predictions\n",
    "    val_pred_log = model.predict(X_val)\n",
    "    val_pred = np.expm1(val_pred_log).clip(0, None)  # Inverse log transform\n",
    "    \n",
    "    # Calculate RMSLE (competition metric)\n",
    "    rmsle = np.sqrt(mean_squared_log_error(y_val, val_pred))\n",
    "    mlflow.log_metric(\"rmsle\", rmsle)\n",
    "    \n",
    "    print(f\"\\n✓ Model trained successfully\")\n",
    "    print(f\"  • RMSLE: {rmsle:.4f}\")\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "    model_uri = f\"runs:/{run_id}/model\"\n",
    "    print(f\"  • Run ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b25cc13c-b4fc-4210-8f9d-8521cc034994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Model Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac2a7436-93f5-41dc-a349-4a5a9b2a6300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register model in MLflow Model Registry\n",
    "print(\"Registering model...\")\n",
    "\n",
    "model_version = None\n",
    "try:\n",
    "    registered_model = mlflow.register_model(model_uri, MODEL_NAME)\n",
    "    model_version = registered_model.version\n",
    "    print(f\"✓ Model registered: {MODEL_NAME} v{model_version}\")\n",
    "    load_uri = f\"models:/{MODEL_NAME}/{model_version}\"\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Model registry not available: {e}\")\n",
    "    print(\"  Using run URI instead\")\n",
    "    load_uri = model_uri\n",
    "\n",
    "# Load and verify registered model\n",
    "loaded_model = mlflow.pyfunc.load_model(load_uri)\n",
    "val_pred_reloaded = loaded_model.predict(X_val)\n",
    "val_pred_reloaded = np.expm1(val_pred_reloaded).clip(0, None)\n",
    "\n",
    "prediction_diff = np.abs(val_pred - val_pred_reloaded).mean()\n",
    "print(f\"✓ Model loaded and verified\")\n",
    "print(f\"  • Average prediction difference: {prediction_diff:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a2a8229-7d52-4ced-8f56-5130fa82954b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Model Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d427b114-b36c-4fe1-af95-1040d3addf03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare validation results\n",
    "val_results = train_df[val_mask].copy()\n",
    "val_results[\"predicted_sales\"] = val_pred\n",
    "\n",
    "# Daily aggregated results\n",
    "daily_results = val_results.groupby(\"date\")[[\"sales\", \"predicted_sales\"]].sum().reset_index()\n",
    "\n",
    "print(f\"✓ Validation period: {daily_results['date'].min().date()} to {daily_results['date'].max().date()}\")\n",
    "print(f\"  • Total actual sales: ${daily_results['sales'].sum():,.0f}\")\n",
    "print(f\"  • Total predicted sales: ${daily_results['predicted_sales'].sum():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ea33afb-5669-45f8-887a-a21a3754d16a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualization 1: Daily actual vs predicted\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(daily_results[\"date\"], daily_results[\"sales\"], label=\"Actual Sales\", linewidth=2)\n",
    "plt.plot(daily_results[\"date\"], daily_results[\"predicted_sales\"], label=\"Predicted Sales\", linewidth=2, alpha=0.8)\n",
    "plt.title(\"Daily Sales: Actual vs Predicted (Validation Period)\", fontsize=14)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Total Sales ($)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f00fbcb0-0253-48ee-8ea5-fb80580282fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualization 2: Prediction residuals\n",
    "daily_results[\"residual\"] = daily_results[\"sales\"] - daily_results[\"predicted_sales\"]\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(daily_results[\"date\"], daily_results[\"residual\"], linewidth=2, color=\"orange\")\n",
    "plt.axhline(0, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "plt.title(\"Prediction Residuals (Actual - Predicted)\", fontsize=14)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Residual ($)\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Residual statistics:\")\n",
    "print(f\"  • Mean: ${daily_results['residual'].mean():,.2f}\")\n",
    "print(f\"  • Std: ${daily_results['residual'].std():,.2f}\")\n",
    "print(f\"  • Min: ${daily_results['residual'].min():,.2f}\")\n",
    "print(f\"  • Max: ${daily_results['residual'].max():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e416ff6e-c89a-472d-a843-bff2d0e2060c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualization 3: Residual distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(daily_results[\"residual\"], bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "plt.axvline(0, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Zero Error\")\n",
    "plt.title(\"Distribution of Prediction Residuals\", fontsize=14)\n",
    "plt.xlabel(\"Residual ($)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edd80189-c373-43dd-a736-03cdd43505fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualization 4: Weekly aggregation\n",
    "val_results[\"week\"] = val_results[\"date\"].dt.to_period(\"W\").dt.start_time\n",
    "weekly_results = val_results.groupby(\"week\")[[\"sales\", \"predicted_sales\"]].sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(weekly_results[\"week\"], weekly_results[\"sales\"], marker=\"o\", label=\"Actual\", linewidth=2)\n",
    "plt.plot(weekly_results[\"week\"], weekly_results[\"predicted_sales\"], marker=\"s\", label=\"Predicted\", linewidth=2)\n",
    "plt.title(\"Weekly Sales: Actual vs Predicted\", fontsize=14)\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"Total Sales ($)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f175f86-76b1-4962-8bd7-e8fe61acbc7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualization 5: Performance by top product families\n",
    "top_families = val_results.groupby(\"family\")[\"sales\"].sum().nlargest(6).index\n",
    "\n",
    "print(f\"Top {len(top_families)} product families by sales:\")\n",
    "for i, family in enumerate(top_families, 1):\n",
    "    family_sales = val_results[val_results[\"family\"] == family][\"sales\"].sum()\n",
    "    print(f\"  {i}. {family}: ${family_sales:,.0f}\")\n",
    "\n",
    "for family in top_families:\n",
    "    family_data = val_results[val_results[\"family\"] == family].copy()\n",
    "    family_daily = family_data.groupby(\"date\")[[\"sales\", \"predicted_sales\"]].sum().reset_index()\n",
    "    \n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.plot(family_daily[\"date\"], family_daily[\"sales\"], label=\"Actual\", linewidth=2)\n",
    "    plt.plot(family_daily[\"date\"], family_daily[\"predicted_sales\"], label=\"Predicted\", linewidth=2, alpha=0.8)\n",
    "    plt.title(f\"Product Family: {family}\", fontsize=12)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Sales ($)\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "771d9b3e-a971-4b16-ad84-81c758464c92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualization 6: Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"feature\": FEATURE_COLS,\n",
    "    \"importance\": model.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance[\"feature\"], feature_importance[\"importance\"])\n",
    "plt.title(\"Feature Importance\", fontsize=14)\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.grid(axis=\"x\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 most important features:\")\n",
    "for i, row in feature_importance.tail(5)[::-1].iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6e587ac-ec59-46f2-b4f3-52eaa9fa3fe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Generate Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd5fa9f0-d024-4b95-bf5d-f625bdf28dd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate predictions for test set\n",
    "print(\"Generating test set predictions...\")\n",
    "\n",
    "test_pred_log = loaded_model.predict(X_test)\n",
    "test_pred = np.expm1(test_pred_log).clip(0, None)\n",
    "\n",
    "# Prepare predictions dataframe\n",
    "predictions_df = test_df[[\"date\", \"store_nbr\", \"family\"]].copy()\n",
    "predictions_df[\"predicted_sales\"] = test_pred\n",
    "\n",
    "print(f\"✓ Test predictions generated\")\n",
    "print(f\"  • Total predictions: {len(predictions_df):,}\")\n",
    "print(f\"  • Date range: {predictions_df['date'].min().date()} to {predictions_df['date'].max().date()}\")\n",
    "print(f\"  • Predicted total sales: ${test_pred.sum():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9d04297-9896-48de-84e0-3d7c77c69e06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save predictions to Gold layer\n",
    "print(\"Saving predictions to Gold layer...\")\n",
    "\n",
    "predictions_spark = spark.createDataFrame(predictions_df)\n",
    "predictions_spark.write.mode(\"overwrite\").saveAsTable(\"cscie103_catalog.final_project.gold_predictions\")\n",
    "\n",
    "print(\"✓ Predictions saved to gold_predictions table\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING PIPELINE COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"✓ Model: {MODEL_NAME}\")\n",
    "print(f\"✓ RMSLE: {rmsle:.4f}\")\n",
    "print(f\"✓ Predictions: gold_predictions table\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05-mlflow-training",
   "widgets": {}
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 2887556,
     "sourceId": 29781,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
