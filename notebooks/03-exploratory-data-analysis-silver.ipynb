{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dd9df89-c192-4e57-a421-5d080e3d64a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c463218f-dae7-44c4-a1e6-4786df760dba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "VOLUME_ROOT_PATH = \"/Volumes/cscie103_catalog/final_project\"\n",
    "VOLUME_DATA_DIR = f\"{VOLUME_ROOT_PATH}/data\"\n",
    "\n",
    "CATALOG_NAME = \"cscie103_catalog\"\n",
    "SCHEMA_NAME = \"final_project\"\n",
    "spark.sql(f\"USE {CATALOG_NAME}.{SCHEMA_NAME}\")\n",
    "\n",
    "class DataframeNames:\n",
    "    HOLIDAYS = \"holidays\"\n",
    "    OIL = \"oil\"\n",
    "    STORES = \"stores\"\n",
    "    TEST = \"test\"\n",
    "    TRAIN = \"train\"\n",
    "    TRANSACTIONS = \"transactions\"\n",
    "    TRAINING = \"training\"\n",
    "\n",
    "    ALL = [ HOLIDAYS, OIL, STORES, TEST, TRAIN, TRANSACTIONS, TRAINING ]\n",
    "\n",
    "class DataTier:\n",
    "    BRONZE = \"bronze\"\n",
    "    SILVER = \"silver\"\n",
    "    GOLD = \"gold\"\n",
    "\n",
    "    def getBronzeName(tablename):\n",
    "        return DataTier.BRONZE + \"_\" + tablename\n",
    "\n",
    "    def getSilverName(tablename):\n",
    "        return DataTier.SILVER + \"_\" + tablename\n",
    "    \n",
    "    def getGoldName(tablename):\n",
    "        return DataTier.GOLD + \"_\" + tablename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7856694-1c8a-4404-941e-0d911c90a906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extractTransformLoad(bronze_tablename, silver_tablename, transform):\n",
    "    \"\"\"\n",
    "    :param: bronze_tablename - bronze UC table name e.g. bronze_tablename\n",
    "    :param: silver_tablename - silver UC table name e.g. silver_tablename\n",
    "    :param: checkpoint_path - volume path to checkpoint e.g. /Volumes/...\n",
    "    :param: transform - transformation function to apply to bronze table, should accept readStream\n",
    "\n",
    "    :return: streaming query\n",
    "    \"\"\"\n",
    "    print(\"Reading from bronze table: \" + bronze_tablename)\n",
    "    read_stream_df = spark.read.format(\"delta\").table(bronze_tablename)\n",
    "    print(f\"Read {read_stream_df.count()} records from bronze table.\")\n",
    "\n",
    "    print(\"Applying transformation(s)...\")\n",
    "    transformed_df = transform(read_stream_df)\n",
    "    print(f\"After transformation, there are {transformed_df.count()} records.\")\n",
    "\n",
    "    # drop the table beforehands just to be sure that it's a clean go\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {silver_tablename}\")\n",
    "\n",
    "    print(\"Writing to silver table: \" + silver_tablename + \"...\")\n",
    "    transformed_df.write.mode(\"overwrite\").format(\"delta\").mode(\"overwrite\").saveAsTable(silver_tablename)\n",
    "\n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23a442dd-7862-45be-9a09-3b8a28b00d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Building Out Silver Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b4d788-94ce-46a2-83b8-b53e7727644d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9796d5c-3732-4935-98fa-37a5d8daa236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stores Dataframe\n",
    "bronze_tablename_stores = DataTier.getBronzeName(DataframeNames.STORES)\n",
    "silver_tablename_stores = DataTier.getSilverName(DataframeNames.STORES)\n",
    "transform = lambda df: df.dropna()\n",
    "\n",
    "stores_streaming_query = extractTransformLoad(\n",
    "    bronze_tablename_stores,\n",
    "    silver_tablename_stores,\n",
    "    transform\n",
    ")\n",
    "\n",
    "print(\"Stores silver table written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fedbd69-61fa-4936-8100-8ebf2a6192f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa796b58-066e-405c-aec9-89d28510a647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transactions Dataframe\n",
    "bronze_tablename_transactions = DataTier.getBronzeName(DataframeNames.TRANSACTIONS)\n",
    "silver_tablename_transactions = DataTier.getSilverName(DataframeNames.TRANSACTIONS)\n",
    "transform = lambda df: df.dropna()\n",
    "\n",
    "transactions_streaming_query = extractTransformLoad(\n",
    "    bronze_tablename_transactions,\n",
    "    silver_tablename_transactions,\n",
    "    transform\n",
    ")\n",
    "\n",
    "print(\"Transactions silver table written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c15497a-f376-4bd9-9cd3-53fd00e9f0e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "067460e5-dbd5-43f8-b23e-46fc3aac35c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Holidays Dataframe\n",
    "bronze_tablename_holidays = DataTier.getBronzeName(DataframeNames.HOLIDAYS)\n",
    "silver_tablename_holidays = DataTier.getSilverName(DataframeNames.HOLIDAYS)\n",
    "\n",
    "def transform(holidays_events_df):\n",
    "    import pyspark.sql.functions as F\n",
    "    # Preparation of holidays data (holidays_events_df):\n",
    "\n",
    "    # 1. Drop rows with 'transfered' = true -> these were transferred to another date.\n",
    "    #    Identifiable by 'type' = 'Transfer'.\n",
    "    # 2. Explode & Construct\n",
    "    #   a. Explode nationwide holiday to per state, identifiable by 'locale_name' = 'Ecuador'.\n",
    "    #   b. Construct new dataframe with 2 columns: 'date', 'is_holiday' from the holidays df.\n",
    "    # 3. Deduplicate dates. This is made under assumption that all the rest of holiday types are actual holidays.\n",
    "    # 4. Add dates for which 'is_holiday' is 0:\n",
    "    #   a. Add all dates from train_df with 'is_holiday' as 0 and 'state' as 'Ecuador'\n",
    "    #   b. Explode & Construct for each row where 'state' is 'Ecuador' (this time 'is_holiday' = 0)\n",
    "\n",
    "    # 1. Drop rows with 'transfered' = true -> these were transferred to another date.\n",
    "    holidays_events_df = holidays_events_df.where(F.col('locale_name') != 'Transfer')\n",
    "\n",
    "    # 2. Explode & Construct\n",
    "    # retrieve list of all states from stores_df\n",
    "    bronze_tablename_stores = DataTier.getBronzeName(DataframeNames.STORES)\n",
    "    stores_df = spark.read.format(\"delta\").table(bronze_tablename_stores)\n",
    "\n",
    "    ecuador_states = [ row['state'] for row in stores_df.select('state').distinct().collect()]\n",
    "\n",
    "    # add array with all the states to 'Ecuador' rows\n",
    "    holidays_events_df = holidays_events_df.withColumn(\n",
    "        'locale_name_array',\n",
    "        F.when(\n",
    "            F.col('locale_name') == 'Ecuador',\n",
    "            F.array([ F.lit(s) for s in ecuador_states ])\n",
    "        ).otherwise(\n",
    "            F.array(F.col('locale_name'))\n",
    "        )\n",
    "    )\n",
    "    # a, b. Explode & Construct new dataframe with 2 columns: 'date', 'is_holiday'\n",
    "    holidays_events_df = holidays_events_df.select(\n",
    "        'date',\n",
    "        F.explode('locale_name_array').alias('state'),\n",
    "        F.lit(1).alias('is_holiday') \n",
    "    )\n",
    "\n",
    "    # 3. Deduplicate rows by leaving unique per date-state\n",
    "    holidays_events_df = holidays_events_df.dropDuplicates(['date', 'state'])\n",
    "\n",
    "    # 4. Add dates for which 'is_holiday' is 0.\n",
    "    # a. Add all dates from train_df with 'is_holiday' as 0 and 'state' as 'Ecuador'\n",
    "    # read all dates from train\n",
    "    bronze_tablename_train = DataTier.getBronzeName(DataframeNames.TRAIN)\n",
    "    train_df = spark.read.format(\"delta\").table(bronze_tablename_train)\n",
    "    train_dates = train_df.select('date').distinct()\n",
    "\n",
    "    # right join to train_dates -> results in na for new dates for 'is_holiday', 'state'\n",
    "    holidays_events_df = holidays_events_df.join(\n",
    "        train_dates,\n",
    "        on='date',\n",
    "        how='right'\n",
    "    )\n",
    "    # fill na-s\n",
    "    holidays_events_df = holidays_events_df.fillna(0, subset=['is_holiday'])\n",
    "    holidays_events_df = holidays_events_df.fillna('Ecuador', subset=['state'])\n",
    "\n",
    "    # b. Explode & Construct for each row where 'state' is 'Ecuador' (this time 'is_holiday' = 0)\n",
    "    holidays_events_df = holidays_events_df.withColumn(\n",
    "        'locale_name_array',\n",
    "        F.when(\n",
    "            F.col('state') == 'Ecuador',\n",
    "            F.array([ F.lit(s) for s in ecuador_states ])\n",
    "        ).otherwise(\n",
    "            F.array(F.col('state'))\n",
    "        )\n",
    "    )\n",
    "    holidays_events_df = holidays_events_df.select(\n",
    "        'date',\n",
    "        F.explode('locale_name_array').alias('state'),\n",
    "        F.col('is_holiday')\n",
    "    )\n",
    "\n",
    "    return holidays_events_df\n",
    "\n",
    "\n",
    "extractTransformLoad(\n",
    "    bronze_tablename_holidays,\n",
    "    silver_tablename_holidays,\n",
    "    transform\n",
    ")\n",
    "print(\"Holidays silver table written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef56f667-3230-4261-857a-eed9c6b856a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Oil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19f477a4-29f2-4685-a24b-1db905a19bd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Oil Dataframe\n",
    "bronze_tablename_oil = DataTier.getBronzeName(DataframeNames.OIL)\n",
    "silver_tablename_oil = DataTier.getSilverName(DataframeNames.OIL)\n",
    "\n",
    "def transform(oil_df):\n",
    "    import pyspark.sql.functions as F\n",
    "    from pyspark.sql.window import Window\n",
    "    # Preparation of oil data (oil_df):\n",
    "    # 1. Drop rows with 'dcoilwtico' = null -> these are missing oil prices.\n",
    "    # 2. Add missing dates with forward fill.\n",
    "    # 3. Write to silver table.\n",
    "    oil_df = oil_df \\\n",
    "        .dropna(subset=[\"dcoilwtico\"]) \\\n",
    "        .withColumn(\"date\", F.to_date(\"date\")) \\\n",
    "        .withColumn(\"dcoilwtico\", F.col(\"dcoilwtico\").cast(\"double\"))\n",
    "\n",
    "    window_ffill = Window.orderBy(\"date\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "    oil_df = oil_df.withColumn(\n",
    "        \"dcoilwtico\",\n",
    "        F.last(\"dcoilwtico\", ignorenulls=True).over(window_ffill)\n",
    "    )\n",
    "\n",
    "    return oil_df\n",
    "\n",
    "extractTransformLoad(\n",
    "    bronze_tablename_oil,\n",
    "    silver_tablename_oil,\n",
    "    transform\n",
    ")\n",
    "print(\"Oil silver table written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "525f8649-6b35-4ea5-a7f6-883194d5b816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17c40753-eb60-4b7a-ac0c-58bddae3694d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train Dataframe\n",
    "bronze_tablename_train = DataTier.getBronzeName(DataframeNames.TRAIN)\n",
    "silver_tablename_train = DataTier.getSilverName(DataframeNames.TRAIN)\n",
    "transform = lambda df: df.dropna()\n",
    "\n",
    "train_streaming_query = extractTransformLoad(\n",
    "    bronze_tablename_train,\n",
    "    silver_tablename_train,\n",
    "    transform\n",
    ")\n",
    "\n",
    "print(\"Train silver table written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "391961aa-9131-4416-8d94-ebb1b299cf65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Training (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0601f5bb-1290-4811-9a82-aa5788b05d37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Training Dataframe\n",
    "def trainingETL():\n",
    "  import pyspark.sql.functions as F\n",
    "  from pyspark.ml.feature import FeatureHasher\n",
    "\n",
    "  # 1. Merge train & transactions -> training:\n",
    "  #   a. train left join transactions using date, store_nbr\n",
    "  #   b. treat cases where transaction are null for 0 sales for that day (fill as 0)\n",
    "  #   c. drop na (expected 3248 rows)\n",
    "  # 2. Merge training & stores -> training:\n",
    "  #   a. training left join stores using store_nbr, state\n",
    "  # 3. Merge training & holidays -> training:\n",
    "  #   a. training left join holidays using date, state\n",
    "  # 4. Drop na (expected 0)\n",
    "  # 5. Encode categorical features using FeatureHasher\n",
    "\n",
    "  training_df = spark.read.format(\"delta\").table(DataTier.getSilverName(DataframeNames.TRAIN))\n",
    "\n",
    "  # 1. Merge train & transactions -> training:\n",
    "  #   a. train left join transactions using date, store_nbr\n",
    "  #   b. treat cases where transaction are null for 0 sales for that day (fill as 0)\n",
    "  training_df = training_df \\\n",
    "      .join(\n",
    "          spark.read.format(\"delta\").table(DataTier.getSilverName(DataframeNames.TRANSACTIONS)),\n",
    "          on=['date', 'store_nbr'],\n",
    "          how='left'\n",
    "      ) \\\n",
    "      .withColumn(\n",
    "        \"transactions\",\n",
    "        F.when(\n",
    "            (F.col(\"sales\") == 0) & (F.col(\"transactions\").isNull()),\n",
    "            F.lit(0)\n",
    "        ).otherwise(F.col(\"transactions\"))\n",
    "      ) \\\n",
    "      \n",
    "  def smart_na_drop(df):\n",
    "    count_before = df.count()\n",
    "    df = df.dropna()\n",
    "    count_after = df.count()\n",
    "    print(f\"\\nB: {count_before};\\nA: {count_after}\")\n",
    "    print(f\"Dropped {count_before - count_after} rows because of nulls.\")\n",
    "    return df\n",
    "  # expected to drop 3248 rows, for these rows there were no transactions recorded despite sales present\n",
    "  training_df = smart_na_drop(training_df)\n",
    "\n",
    "  # 2. Merge training & stores -> training:\n",
    "  # 3. Merge training & holidays -> training:\n",
    "  training_df = training_df \\\n",
    "        .join(\n",
    "            spark.read.format(\"delta\").table(DataTier.getSilverName(DataframeNames.STORES)),\n",
    "            on='store_nbr',\n",
    "            how='left'\n",
    "        ) \\\n",
    "        .join(\n",
    "            spark.read.format(\"delta\").table(DataTier.getSilverName(DataframeNames.HOLIDAYS)),\n",
    "            on=['date', 'state'],\n",
    "            how='left'\n",
    "        ) \\\n",
    "  \n",
    "  # 4. Drop na (expected 0)\n",
    "  count_before_drop = training_df.count()\n",
    "  training_df = training_df.dropna()\n",
    "  count_after_drop = training_df.count()\n",
    "  print(f\"Dropped {count_after_drop - count_before_drop} rows because of nulls.\\nExpected 0.\\nBefore: {count_before_drop}\\nAfter: {count_after_drop}\")\n",
    "\n",
    "  # 5. Encode categorical features using FeatureHasher\n",
    "  categorical_columns_to_encode = [ 'store_nbr', 'family', 'city', 'state', 'type', 'cluster' ]\n",
    "\n",
    "  holidays_hasher = FeatureHasher(\n",
    "      inputCols=categorical_columns_to_encode,\n",
    "      outputCol='hash_storeNbr_family_city_state_type_cluster',\n",
    "      numFeatures=1024\n",
    "  )\n",
    "  training_df = holidays_hasher.transform(training_df)\n",
    "\n",
    "  # drop table first to ensure everything's clean\n",
    "  spark.sql(f\"DROP TABLE IF EXISTS {DataTier.getSilverName(DataframeNames.TRAINING)}\")\n",
    "  training_df.write.format(\"delta\").saveAsTable(DataTier.getSilverName(DataframeNames.TRAINING))\n",
    "\n",
    "  return training_df\n",
    "    \n",
    "training_df = trainingETL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7298d909-af44-4e89-a2d0-f57d4aec0199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "training_df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4938962754428469,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03-exploratory-data-analysis-silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
