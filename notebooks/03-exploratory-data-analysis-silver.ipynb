{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dd9df89-c192-4e57-a421-5d080e3d64a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c463218f-dae7-44c4-a1e6-4786df760dba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "VOLUME_ROOT_PATH = \"/Volumes/cscie103_catalog/final_project\"\n",
    "VOLUME_DATA_DIR = f\"{VOLUME_ROOT_PATH}/data\"\n",
    "\n",
    "CATALOG_NAME = \"cscie103_catalog\"\n",
    "SCHEMA_NAME = \"final_project\"\n",
    "spark.sql(f\"USE {CATALOG_NAME}.{SCHEMA_NAME}\")\n",
    "\n",
    "class DataframeNames:\n",
    "    HOLIDAYS = \"holidays\"\n",
    "    OIL = \"oil\"\n",
    "    STORES = \"stores\"\n",
    "    TEST = \"test\"\n",
    "    TRAIN = \"train\"\n",
    "    TRANSACTIONS = \"transactions\"\n",
    "    TRAINING = \"training\"\n",
    "\n",
    "    ALL = [ HOLIDAYS, OIL, STORES, TEST, TRAIN, TRANSACTIONS, TRAINING ]\n",
    "\n",
    "class DataTier:\n",
    "    BRONZE = \"bronze\"\n",
    "    SILVER = \"silver\"\n",
    "    GOLD = \"gold\"\n",
    "\n",
    "    def getBronzeName(tablename):\n",
    "        return DataTier.BRONZE + \"_\" + tablename\n",
    "\n",
    "    def getSilverName(tablename):\n",
    "        return DataTier.SILVER + \"_\" + tablename\n",
    "    \n",
    "    def getGoldName(tablename):\n",
    "        return DataTier.GOLD + \"_\" + tablename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7856694-1c8a-4404-941e-0d911c90a906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extractTransformLoad(bronze_tablename, silver_tablename, checkpoint_path, transform):\n",
    "    \"\"\"\n",
    "    :param: bronze_tablename - bronze UC table name e.g. bronze_tablename\n",
    "    :param: silver_tablename - silver UC table name e.g. silver_tablename\n",
    "    :param: checkpoint_path - volume path to checkpoint e.g. /Volumes/...\n",
    "    :param: transform - transformation function to apply to bronze table, should accept readStream\n",
    "\n",
    "    :return: streaming query\n",
    "    \"\"\"\n",
    "    print(\"Reading from bronze table: \" + bronze_tablename)\n",
    "    read_stream_df = spark.readStream.format(\"delta\").table(bronze_tablename)\n",
    "\n",
    "    print(\"Applying transformation(s)...\")\n",
    "    transformed_df = transform(read_stream_df)\n",
    "\n",
    "    print(\"Writing to silver table: \" + silver_tablename + \"...\")\n",
    "    streaming_query = transformed_df.writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"delta\") \\\n",
    "        .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "        .trigger(once=True) \\\n",
    "        .toTable(silver_tablename)\n",
    "\n",
    "    return streaming_query\n",
    "\n",
    "def extractTransformLoadStatic(bronze_tablename, silver_tablename, transform):\n",
    "    read_df = spark.read.format(\"delta\").table(bronze_tablename)\n",
    "    transformed_df = transform(read_df)\n",
    "    transformed_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_tablename)\n",
    "    \n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92763dc8-936c-42c9-b650-8ae87afb1bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# set up checkpoint volume for streaming in Silver Tier\n",
    "VOLUME_CHECKPOINTS_DIR = f\"{VOLUME_DATA_DIR}/checkpoints\"\n",
    "for dataframe_name in DataframeNames.ALL:\n",
    "    checkpoint_path = f\"{VOLUME_CHECKPOINTS_DIR}/{dataframe_name}\"\n",
    "    dbutils.fs.rm(checkpoint_path, True)\n",
    "    dbutils.fs.mkdirs(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23a442dd-7862-45be-9a09-3b8a28b00d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Building Out Silver Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b4d788-94ce-46a2-83b8-b53e7727644d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9796d5c-3732-4935-98fa-37a5d8daa236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stores Dataframe\n",
    "bronze_tablename_stores = DataTier.getBronzeName(DataframeNames.STORES)\n",
    "silver_tablename_stores = DataTier.getSilverName(DataframeNames.STORES)\n",
    "checkpoint_path_stores = f\"{VOLUME_CHECKPOINTS_DIR}/{DataframeNames.STORES}\"\n",
    "transform = lambda df: df.dropna()\n",
    "\n",
    "stores_streaming_query = extractTransformLoad(\n",
    "    bronze_tablename_stores,\n",
    "    silver_tablename_stores,\n",
    "    checkpoint_path_stores,\n",
    "    transform\n",
    ")\n",
    "\n",
    "stores_streaming_query.awaitTermination()\n",
    "print(\"Stores silver table written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fedbd69-61fa-4936-8100-8ebf2a6192f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa796b58-066e-405c-aec9-89d28510a647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transactions Dataframe\n",
    "bronze_tablename_transactions = DataTier.getBronzeName(DataframeNames.TRANSACTIONS)\n",
    "silver_tablename_transactions = DataTier.getSilverName(DataframeNames.TRANSACTIONS)\n",
    "checkpoint_path_transactions = f\"{VOLUME_CHECKPOINTS_DIR}/{DataframeNames.TRANSACTIONS}\"\n",
    "transform = lambda df: df.dropna()\n",
    "\n",
    "transactions_streaming_query = extractTransformLoad(\n",
    "    bronze_tablename_transactions,\n",
    "    silver_tablename_transactions,\n",
    "    checkpoint_path_transactions,\n",
    "    transform\n",
    ")\n",
    "\n",
    "transactions_streaming_query.awaitTermination()\n",
    "print(\"Transactions silver table written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c15497a-f376-4bd9-9cd3-53fd00e9f0e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "067460e5-dbd5-43f8-b23e-46fc3aac35c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Holidays Dataframe\n",
    "bronze_tablename_holidays = DataTier.getBronzeName(DataframeNames.HOLIDAYS)\n",
    "silver_tablename_holidays = DataTier.getSilverName(DataframeNames.HOLIDAYS)\n",
    "\n",
    "def transform(holidays_events_df):\n",
    "    import pyspark.sql.functions as F\n",
    "    # Preparation of holidays data (holidays_events_df):\n",
    "\n",
    "    # 1. Drop rows with 'transfered' = true -> these were transferred to another date.\n",
    "    #    Identifiable by 'type' = 'Transfer'.\n",
    "    # 2. Explode & Construct\n",
    "    #   a. Explode nationwide holiday to per state, identifiable by 'locale_name' = 'Ecuador'.\n",
    "    #   b. Construct new dataframe with 2 columns: 'date', 'is_holiday' from the holidays df.\n",
    "    # 3. Deduplicate dates. This is made under assumption that all the rest of holiday types are actual holidays.\n",
    "    # 4. Add dates for which 'is_holiday' is 0:\n",
    "    #   a. Add all dates from train_df with 'is_holiday' as 0 and 'state' as 'Ecuador'\n",
    "    #   b. Explode & Construct for each row where 'state' is 'Ecuador' (this time 'is_holiday' = 0)\n",
    "\n",
    "    # 1. Drop rows with 'transfered' = true -> these were transferred to another date.\n",
    "    holidays_events_df = holidays_events_df.where(F.col('locale_name') != 'Transfer')\n",
    "\n",
    "    # 2. Explode & Construct\n",
    "    # retrieve list of all states from stores_df\n",
    "    bronze_tablename_stores = DataTier.getBronzeName(DataframeNames.STORES)\n",
    "    stores_df = spark.read.format(\"delta\").table(bronze_tablename_stores)\n",
    "\n",
    "    ecuador_states = [ row['state'] for row in stores_df.select('state').distinct().collect()]\n",
    "\n",
    "    # add array with all the states to 'Ecuador' rows\n",
    "    holidays_events_df = holidays_events_df.withColumn(\n",
    "        'locale_name_array',\n",
    "        F.when(\n",
    "            F.col('locale_name') == 'Ecuador',\n",
    "            F.array([ F.lit(s) for s in ecuador_states ])\n",
    "        ).otherwise(\n",
    "            F.array(F.col('locale_name'))\n",
    "        )\n",
    "    )\n",
    "    # a, b. Explode & Construct new dataframe with 2 columns: 'date', 'is_holiday'\n",
    "    holidays_events_df = holidays_events_df.select(\n",
    "        'date',\n",
    "        F.explode('locale_name_array').alias('state'),\n",
    "        F.lit(1).alias('is_holiday') \n",
    "    )\n",
    "\n",
    "    # 3. Deduplicate rows by leaving unique per date-state\n",
    "    holidays_events_df = holidays_events_df.dropDuplicates(['date', 'state'])\n",
    "\n",
    "    # 4. Add dates for which 'is_holiday' is 0.\n",
    "    # a. Add all dates from train_df with 'is_holiday' as 0 and 'state' as 'Ecuador'\n",
    "    # read all dates from train\n",
    "    bronze_tablename_train = DataTier.getBronzeName(DataframeNames.TRAIN)\n",
    "    train_df = spark.read.format(\"delta\").table(bronze_tablename_train)\n",
    "    train_dates = train_df.select('date').distinct()\n",
    "\n",
    "    # right join to train_dates -> results in na for new dates for 'is_holiday', 'state'\n",
    "    holidays_events_df = holidays_events_df.join(\n",
    "        train_dates,\n",
    "        on='date',\n",
    "        how='right'\n",
    "    )\n",
    "    # fill na-s\n",
    "    holidays_events_df = holidays_events_df.fillna(0, subset=['is_holiday'])\n",
    "    holidays_events_df = holidays_events_df.fillna('Ecuador', subset=['state'])\n",
    "\n",
    "    # b. Explode & Construct for each row where 'state' is 'Ecuador' (this time 'is_holiday' = 0)\n",
    "    holidays_events_df = holidays_events_df.withColumn(\n",
    "        'locale_name_array',\n",
    "        F.when(\n",
    "            F.col('state') == 'Ecuador',\n",
    "            F.array([ F.lit(s) for s in ecuador_states ])\n",
    "        ).otherwise(\n",
    "            F.array(F.col('state'))\n",
    "        )\n",
    "    )\n",
    "    holidays_events_df = holidays_events_df.select(\n",
    "        'date',\n",
    "        F.explode('locale_name_array').alias('state'),\n",
    "        F.col('is_holiday')\n",
    "    )\n",
    "\n",
    "    return holidays_events_df\n",
    "\n",
    "\n",
    "extractTransformLoadStatic(\n",
    "    bronze_tablename_holidays,\n",
    "    silver_tablename_holidays,\n",
    "    transform\n",
    ")\n",
    "print(\"Holidays silver table written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef56f667-3230-4261-857a-eed9c6b856a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Oil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19f477a4-29f2-4685-a24b-1db905a19bd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Oil Dataframe\n",
    "bronze_tablename_oil = DataTier.getBronzeName(DataframeNames.OIL)\n",
    "silver_tablename_oil = DataTier.getSilverName(DataframeNames.OIL)\n",
    "\n",
    "def transform(oil_df):\n",
    "    import pyspark.sql.functions as F\n",
    "    from pyspark.sql.window import Window\n",
    "    # Preparation of oil data (oil_df):\n",
    "    # 1. Drop rows with 'dcoilwtico' = null -> these are missing oil prices.\n",
    "    # 2. Add missing dates with forward fill.\n",
    "    # 3. Write to silver table.\n",
    "    oil_df = oil_df \\\n",
    "        .dropna(subset=[\"dcoilwtico\"]) \\\n",
    "        .withColumn(\"date\", F.to_date(\"date\")) \\\n",
    "        .withColumn(\"dcoilwtico\", F.col(\"dcoilwtico\").cast(\"double\"))\n",
    "\n",
    "    window_ffill = Window.orderBy(\"date\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "    oil_df = oil_df.withColumn(\n",
    "        \"dcoilwtico\",\n",
    "        F.last(\"dcoilwtico\", ignorenulls=True).over(window_ffill)\n",
    "    )\n",
    "\n",
    "    return oil_df\n",
    "\n",
    "extractTransformLoadStatic(\n",
    "    bronze_tablename_oil,\n",
    "    silver_tablename_oil,\n",
    "    transform\n",
    ")\n",
    "print(\"Oil silver table written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "525f8649-6b35-4ea5-a7f6-883194d5b816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17c40753-eb60-4b7a-ac0c-58bddae3694d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train Dataframe\n",
    "bronze_tablename_train = DataTier.getBronzeName(DataframeNames.TRAIN)\n",
    "silver_tablename_train = DataTier.getSilverName(DataframeNames.TRAIN)\n",
    "checkpoint_path = f\"{VOLUME_CHECKPOINTS_DIR}/{DataframeNames.TRAIN}\"\n",
    "transform = lambda df: df.dropna()\n",
    "\n",
    "train_streaming_query = extractTransformLoad(\n",
    "    bronze_tablename_train,\n",
    "    silver_tablename_train,\n",
    "    checkpoint_path,\n",
    "    transform\n",
    ")\n",
    "\n",
    "train_streaming_query.awaitTermination()\n",
    "print(\"Train silver table written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "391961aa-9131-4416-8d94-ebb1b299cf65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Training (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0601f5bb-1290-4811-9a82-aa5788b05d37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Training Dataframe\n",
    "def trainingETL():\n",
    "  pass\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4938962754428469,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03-exploratory-data-analysis-silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
