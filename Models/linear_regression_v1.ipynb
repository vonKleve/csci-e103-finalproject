{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26fcc518-586a-4b9f-ad6e-4111c46dfcc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eb0f420-c5b4-4f50-9ab5-82d59e48ce22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "VOLUME_ROOT_PATH = \"/Volumes/cscie103_catalog/final_project/data\"\n",
    "# place where raw csvs land after download\n",
    "VOLUME_TARGET_DIR = f\"{VOLUME_ROOT_PATH}/raw\"\n",
    "# raw data\n",
    "VOLUME_BRONZE_DIR = f\"{VOLUME_ROOT_PATH}/bronze\"\n",
    "# place where prepared data is written\n",
    "VOLUME_SILVER_DIR = f\"{VOLUME_ROOT_PATH}/silver\"\n",
    "# place where final data is written\n",
    "VOLUME_GOLD_DIR = f\"{VOLUME_ROOT_PATH}/gold\"\n",
    "\n",
    "# ensure all paths exist\n",
    "for path in [VOLUME_TARGET_DIR, VOLUME_BRONZE_DIR, VOLUME_SILVER_DIR, VOLUME_GOLD_DIR]:\n",
    "  if not os.path.exists(path):\n",
    "    os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a842ea7b-55c9-46c9-b7fd-fea4b35504f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read train_df from VOLUME_SILVER_DIR/encoded_train_df\n",
    "data_df = spark.read.parquet(VOLUME_SILVER_DIR + \"/encoded_train_df\")\n",
    "\n",
    "data_df.printSchema()\n",
    "print(f'data_df has {data_df.count()} rows and {len(data_df.columns)} columns\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e95be91-e232-4087-b7f0-52afdd4744b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checking rule of thumb: N > 50 + 8 * k, where N - number of rows, k - number of columns\n",
    "print(f'N = {data_df.count()}, k = {len(data_df.columns)}')\n",
    "print(f'N > 50 + 8 * k is {data_df.count() > 50 + 8 * len(data_df.columns)} ({50 + 8 * len(data_df.columns)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe48b394-5c04-47ea-8abe-2107d8f36509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb021c8e-d2f7-47a4-a77a-dc5948db2024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train / Test split\n",
    "# time series data, cannot perform random split\n",
    "# let's do it for hardcoded year of 2016\n",
    "\n",
    "# train_df - all rows with is_year__2016 = 0 & is_year__2017 = 0\n",
    "# test_df - all rows with is_year__2016 = 1 & is_year__2017 = 0\n",
    "train_df = data_df.filter((data_df['is_year__2016'] == 0) & (data_df['is_year__2017'] == 0))\n",
    "test_df = data_df.filter((data_df['is_year__2016'] == 1) | (data_df['is_year__2017'] == 1))\n",
    "\n",
    "X_train = train_df.drop('total_daily_sales')\n",
    "y_train = train_df.select('total_daily_sales')\n",
    "X_test = test_df.drop('total_daily_sales')\n",
    "y_test = test_df.select('total_daily_sales')\n",
    "\n",
    "print(f'train_df has {X_train.count()} rows and {len(X_train.columns)} columns')\n",
    "print(f'test_df has {X_test.count()} rows and {len(X_test.columns)} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99d68d3a-a79c-415e-9159-cbfd59d6fe65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# to pandas\n",
    "X_train = X_train.toPandas()\n",
    "X_test = X_test.toPandas()\n",
    "y_train = y_train.toPandas()\n",
    "y_test = y_test.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3a0be7d-c5be-443a-9676-7b29f5452c4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## WIP (does not quite work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e11f57a3-3b48-4fe2-8d2d-bc75663bc66f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5466bd75-9234-4015-a54e-206fe288694a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "y_pred = lr_model.predict(X_test).ravel()\n",
    "y_fit = lr_model.predict(X_train).ravel()\n",
    "\n",
    "y_pred = pd.Series(y_pred, index = y_test.index)\n",
    "y_fit = pd.Series(y_fit, index = y_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28abf275-d6d6-4882-af4a-8898f22e08c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# rmsle\n",
    "def rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "print(f'train rmsle = {rmsle(y_train, y_fit)}')\n",
    "print(f'test rmsle = {rmsle(y_test, y_pred)}')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "linear_regression_v1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
