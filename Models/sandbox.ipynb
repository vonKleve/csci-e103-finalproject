{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5313152-cf0e-47ca-bd8c-a9eb3847ca7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f2cdb65-6421-472a-9d01-a74af1d8a9c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the data from local volumes\n",
    "VOLUME_ROOT_PATH = \"/Volumes/cscie103_catalog/final_project/data\"\n",
    "VOLUME_TARGET_DIR = f\"{VOLUME_ROOT_PATH}/raw\"\n",
    "filenames = {\n",
    "    'holidays_events': 'holidays_events.csv',\n",
    "    'oil': 'oil.csv',\n",
    "    'sample_submission': 'sample_submission.csv',\n",
    "    'stores': 'stores.csv',\n",
    "    'test': 'test.csv',\n",
    "    'train': 'train.csv',\n",
    "    'transactions': 'transactions.csv'\n",
    "}\n",
    "\n",
    "holidays_events_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('holidays_events')}\", header=True, inferSchema=True)\n",
    "oil_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('oil')}\", header=True, inferSchema=True)\n",
    "stores_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('stores')}\", header=True, inferSchema=True)\n",
    "transactions_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('transactions')}\", header=True, inferSchema=True)\n",
    "train_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('train')}\", header=True, inferSchema=True)\n",
    "\n",
    "test_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('test')}\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c19153f6-58ac-4378-ace6-ce5a41f56079",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# eda: train\n",
    "def analyze_train(train_df):\n",
    "    \"\"\"\n",
    "    Wrapped into function for convenience, also to not litter env with variables.\n",
    "    \"\"\"\n",
    "    display(train_df.limit(5))\n",
    "    display(train_df.summary())\n",
    "    display(train_df.dtypes)\n",
    "\n",
    "    # store_nbr\n",
    "    print(f\"Number of unique 'store_nbr': {train_df.select('store_nbr').distinct().count()}\")\n",
    "    store_nbr_all = list(map(lambda x: x[0], train_df.select('store_nbr').distinct().collect()))\n",
    "    print(f\"Unique values in 'store_nbr' column: {store_nbr_all}\")\n",
    "    # family\n",
    "    print(f\"Number of unique values in 'family' column: {train_df.select('family').distinct().count()}\")\n",
    "    family_all = list(map(lambda x: x[0], train_df.select('family').distinct().collect()))\n",
    "    print(f\"Unique values in 'family' column: {family_all}\")\n",
    "    # date\n",
    "    date_all = list(map(lambda x: x[0], train_df.select('date').distinct().collect()))\n",
    "    print(f\"Range of 'date' column: {min(date_all)}, {max(date_all)}\")\n",
    "\n",
    "analyze_train(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4bc17db-63fc-439c-ae20-145a187e38d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# eda: stores\n",
    "def analyze_stores(stores_df):\n",
    "    \"\"\"\n",
    "    Wrapped into function for convenience, also to not litter env with variables.\n",
    "    \"\"\"\n",
    "    display(stores_df.limit(5))\n",
    "    display(stores_df.summary())\n",
    "    display(stores_df.dtypes)\n",
    "\n",
    "    city_all = list(map(lambda x: x[0], stores_df.select('city').distinct().collect()))\n",
    "    print(f\"Unique values in 'city' column: {city_all}\")\n",
    "    state_all = list(map(lambda x: x[0], stores_df.select('state').distinct().collect()))\n",
    "    print(f\"Unique values in 'state' column: {state_all}\")\n",
    "    type_all = list(map(lambda x: x[0], stores_df.select('type').distinct().collect()))\n",
    "    print(f\"Unique values in 'type' column: {type_all}\")\n",
    "    cluster_all = list(map(lambda x: x[0], stores_df.select('cluster').distinct().collect()))\n",
    "    print(f\"Unique values in 'cluster' column: {cluster_all}\")\n",
    "\n",
    "analyze_stores(stores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35e0b143-9fac-409d-be63-7c1ca69b4cc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# eda: transactions\n",
    "def analyze_transactions(transactions_df):\n",
    "    \"\"\"\n",
    "    Wrapped into function for convenience, also to not litter env with variables.\n",
    "    \"\"\"\n",
    "    display(transactions_df.limit(5))\n",
    "    display(transactions_df.summary())\n",
    "    display(transactions_df.dtypes)\n",
    "analyze_transactions(transactions_df)\n",
    "\n",
    "def prepare_transactions(transactions_df):\n",
    "    \"\"\"\n",
    "    Transactions has 3 cols: date, store_nbr, transactions\n",
    "    \"\"\"\n",
    "    df = transactions_df.toPandas()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.set_index('date')\n",
    "    df = df.sort_index()\n",
    "\n",
    "    initial_row_count = df.shape[0]\n",
    "    df = df.dropna() # drop rows with missing values\n",
    "    final_row_count = df.shape[0]\n",
    "    \n",
    "    if initial_row_count != final_row_count:\n",
    "        print(f\"REMOVED {initial_row_count - final_row_count} ROWS WITH MISSING VALUES\")\n",
    "          \n",
    "    return df\n",
    "prepped_transactions_df = prepare_transactions(transactions_df)\n",
    "\n",
    "def plot_daily_transactions(prepped_transactions_df):\n",
    "    \"\"\"\n",
    "    54 unique stores, let's draw 6 stores per plot - 9 plots\n",
    "    \"\"\"\n",
    "    store_nbrs = sorted(prepped_transactions_df['store_nbr'].unique())\n",
    "    n_stores = len(store_nbrs)\n",
    "    stores_per_plot = 6\n",
    "    n_plots = int(np.ceil(n_stores / stores_per_plot))\n",
    "\n",
    "    for i in range(n_plots):\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        start = i * stores_per_plot\n",
    "        end = start + stores_per_plot\n",
    "        selected_stores = store_nbrs[start:end]\n",
    "        for store in selected_stores:\n",
    "            store_df = prepped_transactions_df[prepped_transactions_df['store_nbr'] == store]\n",
    "            plt.plot(store_df.index, store_df['transactions'], label=f'Store {store}', alpha=0.7)\n",
    "        plt.title(f'Daily Transactions: Stores {selected_stores}')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Transactions')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        display(plt.gcf())\n",
    "        plt.close()\n",
    "\n",
    "plot_daily_transactions(prepped_transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cee4273-5ace-4ef5-a0e7-816c223c560e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# eda: oil\n",
    "def analyze_oil(oil_df):\n",
    "    \"\"\"\n",
    "    Wrapped into function for convenience, also to not litter env with variables. \n",
    "    \"\"\"\n",
    "    display(oil_df.limit(5))\n",
    "    display(oil_df.summary())\n",
    "    display(oil_df.dtypes)\n",
    "\n",
    "    return oil_df\n",
    "analyze_oil(oil_df)\n",
    "\n",
    "# prepare: oil\n",
    "def prepare_oil(oil_df):\n",
    "    \"\"\"\n",
    "    Oil has 2 cols: date, dcoilwtico\n",
    "    \"\"\"\n",
    "    df = oil_df.toPandas()\n",
    "    # date is not in datetime YYYY-MM-DD format\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "    df = df.set_index('date')\n",
    "    df = df.sort_index()\n",
    "\n",
    "    df = df.asfreq(freq='D') # fill missing dates\n",
    "    # fill missing values, propagate \n",
    "    # last valid observation forward to next valid\n",
    "    df = df.fillna(method='ffill') \n",
    "\n",
    "    df = df.reset_index()\n",
    "    df = spark.createDataFrame(df)\n",
    "\n",
    "    return df\n",
    "prepared_oil_df = prepare_oil(oil_df)\n",
    "\n",
    "def plot_daily_oil_prices(prepared_oil_df):\n",
    "    df = prepared_oil_df.toPandas()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df['date'], df['dcoilwtico'], color='blue', alpha=0.7)\n",
    "    plt.title('Daily Oil Prices')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('dcoilwtico')\n",
    "    plt.tight_layout()\n",
    "    display(plt.gcf())\n",
    "    plt.close()\n",
    "plot_daily_oil_prices(prepared_oil_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f533587e-8193-425d-8e68-9fdfa49ff5df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data preparation for ML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b9156b6-58f1-4b31-a8c1-f6ac6225f12b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "# v0: only on train + transactions data\n",
    "# make sure that train_df and prepped_transactions_df have exactly the same dates, nothing is left out\n",
    "\n",
    "# make sure that train_df and prepped_transactions_df have exactly the same dates sets\n",
    "train_dates = train_df.select('date').distinct().toPandas()['date']\n",
    "transaction_dates = prepped_transactions_df.index.unique()\n",
    "\n",
    "common_dates = set(train_dates).intersection(set(transaction_dates))\n",
    "\n",
    "# if there are dates left out for whatever reason - print warning\n",
    "if len(common_dates) != len(train_dates):\n",
    "    print(f\"WARNING: {len(train_dates) - len(common_dates)} dates left out, out of {len(train_dates)} total dates\")\n",
    "\n",
    "# if there are dates left out for whatever reason - print warning\n",
    "if len(common_dates) != len(transaction_dates):\n",
    "    print(f\"WARNING: {len(transaction_dates) - len(common_dates)} dates left out\")\n",
    "\n",
    "# train_df = train_df.filter(train_df['date'].isin(common_dates))\n",
    "# prepped_transactions_df = prepped_transactions_df[prepped_transactions_df.index.isin(common_dates)]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sandbox",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
