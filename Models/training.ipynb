{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "790a8917-6ba0-4841-a6aa-4e084b6dd797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ff2357-8377-4b07-a5e1-66198f1bf18d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "VOLUME_ROOT_PATH = \"/Volumes/cscie103_catalog/final_project/data\"\n",
    "\n",
    "# place where prepared data is written\n",
    "VOLUME_SILVER_DIR = f\"{VOLUME_ROOT_PATH}/silver\"\n",
    "\n",
    "# ensure all paths exist\n",
    "for path in [VOLUME_SILVER_DIR]:\n",
    "  if not os.path.exists(path):\n",
    "    os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeb7a4c5-6c0c-4d54-823a-c93650eb99e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_filenames = {\n",
    "    'holidays': 'holidays',\n",
    "    'stores': 'stores',\n",
    "    'train': 'train',\n",
    "    'transactions': 'transactions',\n",
    "    'test': 'test'\n",
    "}\n",
    "\n",
    "# read from Bronze tier as Delta tables\n",
    "holidays_df = spark.read.format(\"delta\").load(f\"{VOLUME_SILVER_DIR}/{silver_filenames.get('holidays')}\")\n",
    "stores_df = spark.read.format(\"delta\").load(f\"{VOLUME_SILVER_DIR}/{silver_filenames.get('stores')}\")\n",
    "train_df = spark.read.format(\"delta\").load(f\"{VOLUME_SILVER_DIR}/{silver_filenames.get('train')}\")\n",
    "transactions_df = spark.read.format(\"delta\").load(f\"{VOLUME_SILVER_DIR}/{silver_filenames.get('transactions')}\")\n",
    "# test_df = spark.read.format(\"delta\").load(f\"{VOLUME_SILVER_DIR}/{silver_filenames.get('test')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c70fb24-52ae-45d0-bab9-85ac3023fa55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "holidays_df.printSchema()\n",
    "stores_df.printSchema()\n",
    "train_df.printSchema()\n",
    "transactions_df.printSchema()\n",
    "# test_df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
