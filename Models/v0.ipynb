{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9702e3ce-ad14-4de3-8319-9f527b7940e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# THIS IS WIP, IT DOES NOT WORK FOR NOW\n",
    "# NEED TO SPECIFY THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7951c627-ce92-4338-b89e-8684cf42b67d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame as SparkDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d98a573-c403-42f5-9e1b-41d9af3472c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "VOLUME_ROOT_PATH = \"/Volumes/cscie103_catalog/final_project/data\"\n",
    "# place where raw csvs land after download\n",
    "VOLUME_TARGET_DIR = f\"{VOLUME_ROOT_PATH}/raw\"\n",
    "# place where prepared data is written\n",
    "VOLUME_PREPARED_DIR = f\"{VOLUME_ROOT_PATH}/prepared\"\n",
    "# place where final data is written\n",
    "VOLUME_FINAL_DIR = f\"{VOLUME_ROOT_PATH}/final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f41a575-2bed-4287-88c8-34d5bedfe960",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ensure all directories exist\n",
    "for dir in [VOLUME_TARGET_DIR, VOLUME_PREPARED_DIR, VOLUME_FINAL_DIR]:\n",
    "    os.makedirs(dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6dbf1e7-86af-4557-9061-8cb6c25f01c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the data from local volumes\n",
    "filenames = {\n",
    "    'holidays_events': 'holidays_events.csv',\n",
    "    'oil': 'oil.csv',\n",
    "    'sample_submission': 'sample_submission.csv',\n",
    "    'stores': 'stores.csv',\n",
    "    'test': 'test.csv',\n",
    "    'train': 'train.csv',\n",
    "    'transactions': 'transactions.csv'\n",
    "}\n",
    "\n",
    "# holidays_events_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('holidays_events')}\", header=True, inferSchema=True)\n",
    "# oil_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('oil')}\", header=True, inferSchema=True)\n",
    "stores_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('stores')}\", header=True, inferSchema=True)\n",
    "transactions_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('transactions')}\", header=True, inferSchema=True)\n",
    "train_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('train')}\", header=True, inferSchema=True)\n",
    "\n",
    "test_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('test')}\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "430936f8-55e4-47a2-a6b8-687438541627",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prepare: train_df\n",
    "def smart_na_drop(df):\n",
    "    before = df.count()\n",
    "    df = df.dropna()\n",
    "    after = df.count()\n",
    "    print(f\"dropped {before - after} rows\")\n",
    "    return df\n",
    "train_df = smart_na_drop(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b9b1c75-2b20-49ff-a529-c4b3b0bfb04c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prepare: transactions_df\n",
    "transactions_df = smart_na_drop(transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50af31e5-7838-4f5e-b3c5-0b0851803134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prepare: stores_df\n",
    "stores_df = smart_na_drop(stores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d46884b9-e621-4a21-b8da-ae9ee3199fdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# merge train_df & transactions_df by date & store_nbr\n",
    "train_df = train_df.join(transactions_df, ['date', 'store_nbr'], how='left')\n",
    "# merge metadata - stores_df\n",
    "train_df = train_df.join(stores_df, ['store_nbr'], how='left')\n",
    "\n",
    "# write to parquet\n",
    "train_df.write.mode(\"overwrite\").parquet(f\"{VOLUME_PREPARED_DIR}/train_prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b9a66d8-0f62-4286-ae62-e8c66671674c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764714473404}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(train_df.dtypes)\n",
    "display(train_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70f3e5de-1a16-446f-90e0-74f0fae57ebc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write train_df in /raw\n",
    "train_df.write.mode(\"overwrite\").parquet(f\"{VOLUME_TARGET_DIR}/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78209a3d-5a15-421c-8b3d-06eb83321a32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# reread from /raw\n",
    "train_df = spark.read.parquet(f\"{VOLUME_TARGET_DIR}/train\")\n",
    "train_df = train_df.toPandas()\n",
    "\n",
    "# type casts\n",
    "train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "\n",
    "train_df['store_nbr'] = train_df['store_nbr'].astype('int32')\n",
    "# train_df['transactions'] = train_df['transactions'].astype('int32')\n",
    "train_df['onpromotion'] = train_df['onpromotion'].astype('int32')\n",
    "train_df['sales'] = train_df['sales'].astype('float64')\n",
    "\n",
    "train_df['family'] = train_df['family'].astype('category')\n",
    "train_df['city'] = train_df['city'].astype('category')\n",
    "train_df['state'] = train_df['state'].astype('category')\n",
    "train_df['type'] = train_df['type'].astype('category')\n",
    "train_df['cluster'] = train_df['cluster'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67977e85-b9bd-440c-bcf7-3e9e34038621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(type(train_df))\n",
    "print(train_df.columns.tolist())\n",
    "# print types\n",
    "for col in train_df.columns:\n",
    "    print(col, train_df[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57467d6e-7625-405c-8833-7759ee85332d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4c6d48e-d927-438e-90c4-4372fd80b1b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train_df preparation\n",
    "# drop useless columns\n",
    "train_df.drop(columns=['id'], axis=1, inplace=True)\n",
    "\n",
    "# feature engineering for date\n",
    "# add feature for previous day's sales\n",
    "train_df['dayofweek'] = train_df['date'].dt.dayofweek # Note: Pandas dayofweek is 0 (Monday) to 6 (Sunday)\n",
    "train_df['dayofmonth'] = train_df['date'].dt.day\n",
    "train_df['month'] = train_df['date'].dt.month\n",
    "\n",
    "# Sort the DataFrame first to ensure the .shift() operation works chronologically\n",
    "train_df = train_df.sort_values(by=['store_nbr', 'family', 'date']).reset_index(drop=True)\n",
    "\n",
    "# Define the groups (equivalent to partitionBy) and apply the shift (equivalent to lag(1))\n",
    "train_df['sales_lag_1'] = train_df.groupby(['store_nbr', 'family'])['sales'].shift(1)\n",
    "\n",
    "# 4. Handle rows where lag is null (first day for a store/family pair)\n",
    "# This is equivalent to PySpark's .filter(F.col('sales_lag_1').isNotNull())\n",
    "train_df = train_df.dropna(subset=['sales_lag_1'])\n",
    "\n",
    "train_df = train_df.drop('date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6086675-9193-472b-a46e-056476ab056a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(train_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0b8cb3f-cd63-43aa-aa92-5f4aedc27c7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4e895c2-3f64-4ad2-b672-4074a00f114e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- 0. Define Feature Groups (Based on your input) ---\n",
    "CATEGORICAL_COLS = ['store_nbr', 'family', 'city', 'state', 'type', 'cluster', \n",
    "                    'dayofweek', 'dayofmonth', 'month']\n",
    "CONTINUOUS_COLS = ['onpromotion', 'transactions', 'sales_lag_1']\n",
    "TARGET_COL = 'sales'\n",
    "\n",
    "\n",
    "# --- 1. Data Preparation and Chronological Split ---\n",
    "\n",
    "# 1a. Sort Data Chronologically (CRITICAL for time series)\n",
    "# Assuming 'date' is still available and is datetime type\n",
    "# train_df = train_df.sort_values(by=['date', 'store_nbr', 'family']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# 1b. Integer Encode Categorical Features\n",
    "vocab_sizes = {}\n",
    "for col in CATEGORICAL_COLS:\n",
    "    # Convert to category type and get codes (0-indexed integers)\n",
    "    train_df[col] = train_df[col].astype('category').cat.codes\n",
    "    # Calculate vocabulary size (number of unique codes)\n",
    "    vocab_sizes[col] = len(train_df[col].unique())\n",
    "\n",
    "\n",
    "# 1c. Scale Continuous Features\n",
    "scaler = StandardScaler()\n",
    "train_df[CONTINUOUS_COLS] = scaler.fit_transform(train_df[CONTINUOUS_COLS])\n",
    "\n",
    "\n",
    "# 1d. Define Split Point (80% Train, 20% Validation)\n",
    "split_index = int(len(train_df) * 0.8)\n",
    "train_set = train_df.iloc[:split_index]\n",
    "val_set = train_df.iloc[split_index:]\n",
    "\n",
    "\n",
    "# 1e. Prepare Input Arrays for Keras Model\n",
    "def prepare_keras_inputs(df):\n",
    "    \"\"\"Generates the list of input arrays for the Keras model.\"\"\"\n",
    "    X_cat = [df[col].values for col in CATEGORICAL_COLS]\n",
    "    X_cont = df[CONTINUOUS_COLS].values\n",
    "    y = df[TARGET_COL].values\n",
    "    \n",
    "    X_inputs = X_cat + [X_cont]\n",
    "    return X_inputs, y\n",
    "\n",
    "X_train_inputs, y_train = prepare_keras_inputs(train_set)\n",
    "X_val_inputs, y_val = prepare_keras_inputs(val_set)\n",
    "\n",
    "print(f\"Data Split Complete. Training samples: {len(y_train)}, Validation samples: {len(y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8585b72-5617-494c-876e-259098a98567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 2. FFNN Model Definition with Embedding Layers ---\n",
    "\n",
    "# Function to determine optimal embedding dimension (common heuristic)\n",
    "def get_embedding_dim(vocab_size):\n",
    "    return min(50, (vocab_size // 2) + 1)\n",
    "\n",
    "input_layers = []\n",
    "cat_embeddings = []\n",
    "\n",
    "# Process Categorical Inputs\n",
    "for col in CATEGORICAL_COLS:\n",
    "    vocab_size = vocab_sizes[col]\n",
    "    emb_dim = get_embedding_dim(vocab_size)\n",
    "    \n",
    "    # 2a. Input Layer\n",
    "    input_layer = Input(shape=(1,), name=f'{col}_input')\n",
    "    input_layers.append(input_layer)\n",
    "\n",
    "    # 2b. Embedding Layer (input_dim = vocab size + 1 for 0-indexing/padding)\n",
    "    embedding = Embedding(\n",
    "        input_dim=vocab_size + 1, \n",
    "        output_dim=emb_dim, \n",
    "        name=f'{col}_embed'\n",
    "    )(input_layer)\n",
    "    \n",
    "    # 2c. Flatten the output\n",
    "    flat = Flatten(name=f'{col}_flat')(embedding)\n",
    "    cat_embeddings.append(flat)\n",
    "\n",
    "# Process Continuous Inputs\n",
    "N_CONT_FEATURES = len(CONTINUOUS_COLS)\n",
    "cont_input = Input(shape=(N_CONT_FEATURES,), name='cont_input')\n",
    "input_layers.append(cont_input) \n",
    "\n",
    "# Concatenate all features\n",
    "merged = concatenate(cat_embeddings + [cont_input])\n",
    "\n",
    "# Define the main FFNN layers\n",
    "dense1 = Dense(128, activation='relu')(merged)\n",
    "dense2 = Dense(64, activation='relu')(dense1)\n",
    "\n",
    "# Output Layer for Sales Prediction (Regression)\n",
    "output = Dense(1, activation='linear')(dense2)\n",
    "\n",
    "# Create and Compile the final Model\n",
    "model = Model(\n",
    "    inputs=input_layers, \n",
    "    outputs=output\n",
    ")\n",
    "\n",
    "# Use Adam optimizer and Mean Squared Error (MSE) loss for regression\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "print(\"--- FFNN Model Architecture ---\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c23f1b92-baf7-43d0-879d-481fb4eb86cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 3. Model Training ---\n",
    "\n",
    "print(\"\\n--- Starting Model Training ---\")\n",
    "\n",
    "# Train the model using the chronologically split data\n",
    "history = model.fit(\n",
    "    X_train_inputs, \n",
    "    y_train, \n",
    "    validation_data=(X_val_inputs, y_val), \n",
    "    epochs=10, \n",
    "    batch_size=64, # Using a reasonable batch size\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n--- Model Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1e98e5a-2f88-49d7-88a8-4ecaa1b7428b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "v0",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
