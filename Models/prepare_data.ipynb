{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95edd353-2518-4745-bb1d-e79b8c9ccbc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Preparation for Linear Regression\n",
    "This notebook outlines the data preparation steps for a linear regression analysis. It organizes the workflow into different tiers:\n",
    "\n",
    "### Bronze Tier\n",
    "Tables as they are\n",
    "\n",
    "### Silver Tier\n",
    "The Silver Tier contains curated, cleaned, and joined data.\n",
    "\n",
    "#### Table `encoded_train_df`:  \n",
    "Numerical variables: `total_daily_sales`, `days_since_earliest_date`, `transactions`, `onpromotion`.  \n",
    "\n",
    "Categorical variables: `store_nbr`, `city`, `state`, `type`, `cluster`, `day_of_week`, `day_of_month`, `month`, `year`.  \n",
    "All categorical variables are one-hot encoded using prefix `is_<varname>_` e.g. `is_state__Ohio` (note the double underscore).\n",
    "\n",
    "\n",
    "This tier is designed for analysis and modeling, providing a structured dataset with relevant features for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8ab9df2-7ff6-40eb-b71a-30d62d95eb1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame as SparkDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbdfca57-f68b-42cc-8d5d-fd7d8c330067",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "VOLUME_ROOT_PATH = \"/Volumes/cscie103_catalog/final_project/data\"\n",
    "# place where raw csvs land after download\n",
    "VOLUME_TARGET_DIR = f\"{VOLUME_ROOT_PATH}/raw\"\n",
    "# raw data\n",
    "VOLUME_BRONZE_DIR = f\"{VOLUME_ROOT_PATH}/bronze\"\n",
    "# place where prepared data is written\n",
    "VOLUME_SILVER_DIR = f\"{VOLUME_ROOT_PATH}/silver\"\n",
    "# place where final data is written\n",
    "VOLUME_GOLD_DIR = f\"{VOLUME_ROOT_PATH}/gold\"\n",
    "\n",
    "# ensure all paths exist\n",
    "for path in [VOLUME_TARGET_DIR, VOLUME_BRONZE_DIR, VOLUME_SILVER_DIR, VOLUME_GOLD_DIR]:\n",
    "  if not os.path.exists(path):\n",
    "    os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "106b229a-9aaf-4639-9fa2-ce78ee51c8dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the data from local volumes\n",
    "filenames = {\n",
    "    'holidays_events': 'holidays_events.csv',\n",
    "    'oil': 'oil.csv',\n",
    "    'sample_submission': 'sample_submission.csv',\n",
    "    'stores': 'stores.csv',\n",
    "    'test': 'test.csv',\n",
    "    'train': 'train.csv',\n",
    "    'transactions': 'transactions.csv'\n",
    "}\n",
    "\n",
    "# holidays_events_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('holidays_events')}\", header=True, inferSchema=True)\n",
    "# oil_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('oil')}\", header=True, inferSchema=True)\n",
    "stores_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('stores')}\", header=True, inferSchema=True)\n",
    "transactions_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('transactions')}\", header=True, inferSchema=True)\n",
    "train_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('train')}\", header=True, inferSchema=True)\n",
    "\n",
    "test_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('test')}\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8279c7e-90c3-4912-a20f-af1ff312e229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bronze Tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89bed661-0442-47c7-98b8-ceaacb24f372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write all dfs as they are into bronze\n",
    "for df, name in zip([stores_df, transactions_df, train_df, test_df], ['stores', 'transactions', 'train', 'test']):\n",
    "  df.write.mode(\"overwrite\").parquet(f\"{VOLUME_BRONZE_DIR}/{name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fe524d6-19d6-434d-9625-5fdf2cec26c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver Tier\n",
    "\n",
    "Produce & persist table:  \n",
    "\n",
    "store_nbr\t|   int  \n",
    "date\t    |   date  \n",
    "id\t        |   int  \n",
    "family\t    |   string  \n",
    "sales\t    |   double  \n",
    "onpromotion\t|   int  \n",
    "transactions|\tint  \n",
    "city\t    |   string  \n",
    "state\t    |   string  \n",
    "type\t    |   string  \n",
    "cluster\t    |   int  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afa6ced0-fee7-4372-94db-530076b71e00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prepare: train_df\n",
    "def smart_na_drop(df):\n",
    "    \"\"\"\n",
    "    Drops all rows with any null values in columns.\n",
    "    \"\"\"\n",
    "    before = df.count()\n",
    "    df = df.dropna()\n",
    "    after = df.count()\n",
    "    print(f\"dropped {before - after} rows\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba1b4806-b4c8-4fcb-a69b-569b128f091b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = smart_na_drop(train_df)\n",
    "transactions_df = smart_na_drop(transactions_df)\n",
    "stores_df = smart_na_drop(stores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "917ff48c-70e5-4d37-b031-89ba5fbd5167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "# This shows the rows which are dropped in the cell below after merging with transactions\n",
    "test_df = train_df\n",
    "test_df = test_df.join(transactions_df, on=['date', 'store_nbr'], how='left')\n",
    "test_df = test_df.withColumn(\n",
    "    'transactions',\n",
    "    F.when(F.col('sales') == 0, 0).otherwise(F.col('transactions'))\n",
    ")\n",
    "# show only rows where nulls are present\n",
    "test_df.where(F.col('transactions').isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9ef1b1f-30c6-43ff-b324-3010d379f72f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Merge with transactions data\n",
    "#       .a Fill transactions as 0 when total_daily_sales is 0\n",
    "#       .b Drop rows where any column is null\n",
    "# 2. Aggregate daily sales across all product families per store_nbr into total_daily_sales\n",
    "# 3. Merge with stores_df\n",
    "\n",
    "strain_df = train_df\n",
    "\n",
    "# 1. Merge with transactions data\n",
    "strain_df = strain_df.join(transactions_df, on=['date', 'store_nbr'], how='left')\n",
    "strain_df = strain_df.withColumn(\n",
    "    'transactions',\n",
    "    F.when(F.col('sales') == 0, 0).otherwise(F.col('transactions'))\n",
    ")\n",
    "strain_df = smart_na_drop(strain_df) # expected to drop 3248 rows\n",
    "\n",
    "# 2. Aggregate daily sales across all product families per store_nbr into total_daily_sales\n",
    "strain_df = strain_df.groupBy('date', 'store_nbr').agg(\n",
    "    F.sum('sales').alias('total_daily_sales'),\n",
    "    F.sum('onpromotion').alias('onpromotion'),\n",
    "    F.sum('transactions').alias('transactions')\n",
    ")\n",
    "\n",
    "# 3. Merge with stores_df\n",
    "strain_df = strain_df.join(stores_df, ['store_nbr'], how='left')\n",
    "strain_df = smart_na_drop(strain_df) # expected to drop 0 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "158c7375-0e2e-4e1b-9064-527de41ef4de",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764797752466}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Schema of strain_df\n",
    "strain_df.printSchema()\n",
    "\n",
    "strain_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17753d91-df43-484f-8fba-ff9eefe68c61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Add columns day_of_week, day, month, year\n",
    "# 2. Add column days_since_earliest_date as number of days since earliest date\n",
    "# 3. Drop date column\n",
    "\n",
    "# 1. Add columns day_of_week, day, month, year\n",
    "strain_df = strain_df.withColumn('day_of_week', F.dayofweek(F.col('date')))\n",
    "strain_df = strain_df.withColumn('day_of_month', F.dayofmonth(F.col('date')))\n",
    "strain_df = strain_df.withColumn('month', F.month(F.col('date')))\n",
    "strain_df = strain_df.withColumn('year', F.year(F.col('date')))\n",
    "\n",
    "# 2. Add column time_since_earliest_date\n",
    "earliest_date = strain_df.select(F.min('date')).collect()[0][0] # Normally is 2013-01-01\n",
    "strain_df = strain_df.withColumn(\n",
    "    'days_since_earliest_date',\n",
    "    F.datediff(F.col('date'), F.lit(earliest_date))\n",
    ")\n",
    "\n",
    "# 3. Drop date column\n",
    "strain_df = strain_df.drop('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de11008-82a6-4d4a-9849-be814d51f359",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764798167265}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparation of the data for Logistic Regression\n",
    "# 1. One-hot encode all categorical\n",
    "#  variables: store_nbr, city, state, type, cluster, day_of_week, day_of_month, month, year\n",
    "\n",
    "setrain_df = strain_df.toPandas()\n",
    "\n",
    "# 1. One-hot encode all categorical variables: store_nbr, city, state, type, cluster, day_of_week, day_of_month, month, year\n",
    "for colname in ['store_nbr', 'city', 'state', 'type', 'cluster', 'day_of_week', 'day_of_month', 'month', 'year']:\n",
    "    setrain_df = pd.get_dummies(\n",
    "        setrain_df,\n",
    "        columns=[colname],\n",
    "        dtype=int,\n",
    "        prefix=f'is_{colname}_'\n",
    "    )\n",
    "display(setrain_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc9e8e19-38de-44e8-a16d-dbe973860e7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Convert setrain_df back to spark dataframe\n",
    "# 2. Write setrain_df to silver table\n",
    "\n",
    "# 1. Convert setrain_df back to spark dataframe\n",
    "setrain_df = spark.createDataFrame(setrain_df)\n",
    "\n",
    "# 2. Write setrain_df to silver table\n",
    "setrain_df.write.mode(\"overwrite\").parquet(f\"{VOLUME_SILVER_DIR}/encoded_train_df\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "prepare_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
