{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b85acec-13ea-4df2-8a9c-cb9040c29e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.ml.feature import FeatureHasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3cfc2d2-f6af-43ac-8d15-0d742a8968cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "VOLUME_ROOT_PATH = \"/Volumes/cscie103_catalog/final_project/data\"\n",
    "# raw data\n",
    "VOLUME_BRONZE_DIR = f\"{VOLUME_ROOT_PATH}/bronze\"\n",
    "# place where prepared data is written\n",
    "VOLUME_SILVER_DIR = f\"{VOLUME_ROOT_PATH}/silver\"\n",
    "\n",
    "# ensure all paths exist\n",
    "for path in [VOLUME_BRONZE_DIR, VOLUME_SILVER_DIR]:\n",
    "  if not os.path.exists(path):\n",
    "    os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0343b46-e14f-4f53-9746-443f1def815b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the data from local volumes\n",
    "tablename = 'holidays'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d0a413-42c5-4677-a51c-4816dc742e39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read from Bronze tier as Delta table\n",
    "bronze_path = f\"{VOLUME_BRONZE_DIR}/{tablename}\"\n",
    "holidays_events_df = spark.read.format(\"delta\").load(bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad2e0048-a4c7-46d7-91b4-a959b09ca5d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "holidays_events_df.printSchema()\n",
    "display(holidays_events_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c312908f-c65e-44c8-b8f1-3ee86dcbf37b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def rows_to_value(df, name):\n",
    "    return [ row[name] for row in df ]\n",
    "\n",
    "display(rows_to_value(holidays_events_df.select('locale_name').distinct().collect(), 'locale_name'))\n",
    "holidays_events_df.select('locale').distinct().show()\n",
    "holidays_events_df.select('type').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9fcad3a-b479-404e-9be3-6538a07373da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preparation of holidays data (holidays_events_df):\n",
    "# 1. Drop rows with 'transfered' = true -> these were transferred to another date.\n",
    "#    Identifiable by 'type' = 'Transfer'\n",
    "# 2. Explode nationwade holiday to per state, identifiable by 'locale_name' = 'Ecuador'\n",
    "# 3. Deduplicate dates. This is made under assumption that all the rest of holiday types are actual holidays.\n",
    "# 4. Construct new dataframe with 2 columns: 'date', 'is_holiday' from the holidays df\n",
    "\n",
    "# 1. Drop rows with 'transfered' = true -> these were transferred to another date.\n",
    "holidays_events_df = holidays_events_df.where(F.col('locale_name') != 'Transfer')\n",
    "\n",
    "# 2. Explode nationwade holiday to per state, identifiable by 'locale_name' = 'Ecuador'\n",
    "# list of states is provided by the stores_df\n",
    "stores_bronze_path = f\"{VOLUME_BRONZE_DIR}/stores\"\n",
    "stores_df = spark.read.format(\"delta\").load(stores_bronze_path)\n",
    "\n",
    "ecuador_states = [ row['state'] for row in stores_df.select('state').distinct().collect()]\n",
    "\n",
    "# add array with all the states to 'Ecuador' rows\n",
    "holidays_events_df = holidays_events_df.withColumn(\n",
    "    'locale_name_array',\n",
    "    F.when(\n",
    "        F.col('locale_name') == 'Ecuador',\n",
    "        F.array([ F.lit(s) for s in ecuador_states ])\n",
    "    ).otherwise(\n",
    "        F.array(F.col('locale_name'))\n",
    "    )\n",
    ")\n",
    "# Explode & \n",
    "# 4. Construct new dataframe with 2 columns: 'date', 'is_holiday' from the holidays df\n",
    "holidays_events_df = holidays_events_df.select(\n",
    "    'date',\n",
    "    F.explode('locale_name_array').alias('state'),\n",
    "    F.lit(1).alias('is_holiday') \n",
    ")\n",
    "\n",
    "# 3. Deduplicate rows by leaving unique per date-state\n",
    "holidays_events_df = holidays_events_df.dropDuplicates(['date', 'state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4eb2986-2665-4e49-a4be-8762d9e1eca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "holidays_events_df.printSchema()\n",
    "display(holidays_events_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39076daf-ed3e-4a28-92b3-b011e8e54e59",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765313571736}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# hash 'state' column\n",
    "# add one fake row at the end with 'is_holiday' = 0, for proper hashing\n",
    "holidays_events_df = holidays_events_df.union(\n",
    "    holidays_events_df.select(\n",
    "        F.lit('2099-01-01').alias('date'),\n",
    "        F.lit('Bolivar').alias('state'),\n",
    "        F.lit(0).alias('is_holiday')\n",
    "    )\n",
    ")\n",
    "\n",
    "holidays_hasher = FeatureHasher(\n",
    "    inputCols=['state', 'is_holiday'],\n",
    "    outputCol='hash_state_isHoliday',\n",
    "    numFeatures=1024\n",
    ")\n",
    "holidays_events_df = holidays_hasher.transform(holidays_events_df)\n",
    "\n",
    "# drop 'state', 'is_holiday' columns\n",
    "holidays_events_df = holidays_events_df.drop('state', 'is_holiday')\n",
    "\n",
    "display(holidays_events_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5e7b4d5-5d40-4afa-bcf2-2fede2711660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write to Silver tier as Delta table\n",
    "silver_path = f\"{VOLUME_SILVER_DIR}/{tablename}\"\n",
    "dbutils.fs.rm(silver_path, True)\n",
    "holidays_events_df.write.mode('overwrite').format(\"delta\").save(silver_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_holidays",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
