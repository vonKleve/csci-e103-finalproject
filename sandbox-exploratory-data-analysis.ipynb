{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87ef88d2-faa1-41c0-9a41-177dec2356d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook serves as a sandbox to provide exploratory data analysis in preparation for ERD & E2E workflow specifications.\n",
    "It will:\n",
    "1. Create schema & volume if needed.\n",
    "2. Fetch data from [Kaggle competition](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview).\n",
    "3. Create respective tables per csv file.\n",
    "\n",
    "... (wip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd588b5c-dba1-41e5-896f-ca05a842e98a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "NoDirective"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- creates cscie103_catalog.final_project schema and data volume (if not exist)\n",
    "CREATE SCHEMA IF NOT EXISTS cscie103_catalog.final_project;\n",
    "CREATE VOLUME IF NOT EXISTS cscie103_catalog.final_project.data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c90597be-fc72-4f28-862f-e0d19632c61b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /local_disk0/.ephemeral_nfs/envs/pythonEnv-30f5661a-ca71-4939-878a-8dd8e8431bd8/lib/python3.12/site-packages (1.8.2)\nRequirement already satisfied: black>=24.10.0 in /databricks/python3/lib/python3.12/site-packages (from kaggle) (24.10.0)\nRequirement already satisfied: bleach in /databricks/python3/lib/python3.12/site-packages (from kaggle) (6.2.0)\nRequirement already satisfied: kagglesdk in /local_disk0/.ephemeral_nfs/envs/pythonEnv-30f5661a-ca71-4939-878a-8dd8e8431bd8/lib/python3.12/site-packages (from kaggle) (0.1.13)\nRequirement already satisfied: mypy>=1.15.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-30f5661a-ca71-4939-878a-8dd8e8431bd8/lib/python3.12/site-packages (from kaggle) (1.19.0)\nRequirement already satisfied: protobuf in /databricks/python3/lib/python3.12/site-packages (from kaggle) (5.29.4)\nRequirement already satisfied: python-dateutil in /databricks/python3/lib/python3.12/site-packages (from kaggle) (2.9.0.post0)\nRequirement already satisfied: python-slugify in /local_disk0/.ephemeral_nfs/envs/pythonEnv-30f5661a-ca71-4939-878a-8dd8e8431bd8/lib/python3.12/site-packages (from kaggle) (8.0.4)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (from kaggle) (2.32.3)\nRequirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (74.0.0)\nRequirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from kaggle) (1.16.0)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-30f5661a-ca71-4939-878a-8dd8e8431bd8/lib/python3.12/site-packages (from kaggle) (4.67.1)\nRequirement already satisfied: types-requests in /local_disk0/.ephemeral_nfs/envs/pythonEnv-30f5661a-ca71-4939-878a-8dd8e8431bd8/lib/python3.12/site-packages (from kaggle) (2.32.4.20250913)\nRequirement already satisfied: types-tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-30f5661a-ca71-4939-878a-8dd8e8431bd8/lib/python3.12/site-packages (from kaggle) (4.67.0.20250809)\nRequirement already satisfied: urllib3>=1.15.1 in /databricks/python3/lib/python3.12/site-packages (from kaggle) (2.3.0)\nRequirement already satisfied: click>=8.0.0 in /databricks/python3/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (8.1.7)\nRequirement already satisfied: mypy-extensions>=0.4.3 in /databricks/python3/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (1.0.0)\nRequirement already satisfied: packaging>=22.0 in /databricks/python3/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (24.1)\nRequirement already satisfied: pathspec>=0.9.0 in /databricks/python3/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (0.10.3)\nRequirement already satisfied: platformdirs>=2 in /databricks/python3/lib/python3.12/site-packages (from black>=24.10.0->kaggle) (3.10.0)\nRequirement already satisfied: typing_extensions>=4.6.0 in /databricks/python3/lib/python3.12/site-packages (from mypy>=1.15.0->kaggle) (4.12.2)\nRequirement already satisfied: librt>=0.6.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-30f5661a-ca71-4939-878a-8dd8e8431bd8/lib/python3.12/site-packages (from mypy>=1.15.0->kaggle) (0.6.3)\nRequirement already satisfied: webencodings in /databricks/python3/lib/python3.12/site-packages (from bleach->kaggle) (0.5.1)\nRequirement already satisfied: text-unidecode>=1.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-30f5661a-ca71-4939-878a-8dd8e8431bd8/lib/python3.12/site-packages (from python-slugify->kaggle) (1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->kaggle) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->kaggle) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->kaggle) (2025.1.31)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ebde8b8-e1db-4d04-95fa-e292533a6cf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%py\n",
    "# all imports here\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5f8062e-11e3-43ca-93fe-f334f95cb4ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%py\n",
    "# Kaggle authentication set up\n",
    "kaggle_token = dbutils.secrets.get(scope=\"e-103-finalproject-credentials\", key=\"kaggle-api-token\")\n",
    "kaggle_username = dbutils.secrets.get(scope=\"e-103-finalproject-credentials\", key=\"kaggle-username\")\n",
    "\n",
    "kaggle_config = { \"username\": kaggle_username, \"key\": kaggle_token }\n",
    "kaggle_config_dir = \"/tmp/kaggle_config\"\n",
    "kaggle_file_path = Path(kaggle_config_dir) / \"kaggle.json\"\n",
    "\n",
    "os.makedirs(kaggle_config_dir, exist_ok=True)\n",
    "with open(kaggle_file_path, \"w\") as f:\n",
    "    json.dump(kaggle_config, f)\n",
    "os.chmod(kaggle_file_path, 0o600)\n",
    "\n",
    "os.environ[\"KAGGLE_CONFIG_DIR\"] = kaggle_config_dir\n",
    "os.environ[\"KAGGLE_USERNAME\"] = kaggle_username\n",
    "os.environ[\"KAGGLE_KEY\"] = kaggle_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c567f915-2949-499b-ae3b-ca5bb3b1c301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# only after the Kaggle auth setup\n",
    "import kaggle as kgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7df8bf4a-9b18-4022-acc3-6baf74eb4bf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped downloading data because file-marker exists already (train.csv).\n\nVerification: Data loaded into Spark DataFrame.\nroot\n |-- id: integer (nullable = true)\n |-- date: date (nullable = true)\n |-- store_nbr: integer (nullable = true)\n |-- family: string (nullable = true)\n |-- sales: double (nullable = true)\n |-- onpromotion: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "%py\n",
    "COMPETITION_NAME = \"store-sales-time-series-forecasting\"\n",
    "VOLUME_ROOT_PATH = \"/Volumes/cscie103_catalog/final_project/data\"\n",
    "VOLUME_TARGET_DIR = f\"{VOLUME_ROOT_PATH}/raw\"\n",
    "DOWNLOAD_PATH = VOLUME_TARGET_DIR\n",
    "\n",
    "# download data only if it does not exist in the VOLUME_TARGET_DIR, marker is train.csv file\n",
    "if not os.path.exists(f\"{VOLUME_TARGET_DIR}/train.csv\"):\n",
    "    print(\"Downloading data...\")\n",
    "\n",
    "    os.makedirs(VOLUME_TARGET_DIR, exist_ok=True)\n",
    "    os.makedirs(DOWNLOAD_PATH, exist_ok=True)\n",
    "\n",
    "    # kaggle competitions download -c store-sales-time-series-forecasting -p <path>\n",
    "    command = [\n",
    "        \"kaggle\", \"competitions\", \"download\", \n",
    "        \"-c\", COMPETITION_NAME, \n",
    "        \"-p\", DOWNLOAD_PATH\n",
    "    ]\n",
    "\n",
    "    # --- Download data ---\n",
    "    try:\n",
    "        result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(\"Download successful.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during download: {e}\")\n",
    "        print(f\"Stdout: {e.stdout}\")\n",
    "        print(f\"Stderr: {e.stderr}\")\n",
    "        raise\n",
    "\n",
    "    # --- Unzip ---\n",
    "    zip_file_name = f\"{DOWNLOAD_PATH}/{COMPETITION_NAME}.zip\"\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
    "            zip_ref.extractall(DOWNLOAD_PATH)\n",
    "            print(f\"Extracted files from {zip_file_name}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: ZIP file not found at {zip_file_name}. Download may have failed.\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"Skipped downloading data because file-marker exists already (train.csv).\")\n",
    "\n",
    "file_path = f\"{VOLUME_TARGET_DIR}/train.csv\"\n",
    "try:\n",
    "    df = (spark.read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .csv(file_path)\n",
    "    )\n",
    "    print(\"\\nVerification: Data loaded into Spark DataFrame.\")\n",
    "    df.printSchema()\n",
    "    # df.display()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading file from Volume: {e}. Check file path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "741546d6-a1fd-41de-b681-bd579238b3e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filenames = {\n",
    "    'holidays_events': 'holidays_events.csv',\n",
    "    'oil': 'oil.csv',\n",
    "    'sample_submission': 'sample_submission.csv',\n",
    "    'stores': 'stores.csv',\n",
    "    'test': 'test.csv',\n",
    "    'train': 'train.csv',\n",
    "    'transactions': 'transactions.csv'\n",
    "}\n",
    "\n",
    "holidays_events_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('holidays_events')}\", header=True, inferSchema=True)\n",
    "oil_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('oil')}\", header=True, inferSchema=True)\n",
    "stores_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('stores')}\", header=True, inferSchema=True)\n",
    "transactions_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('transactions')}\", header=True, inferSchema=True)\n",
    "train_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('train')}\", header=True, inferSchema=True)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7753798376578748,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "sandbox-exploratory-data-analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}