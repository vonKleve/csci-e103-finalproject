{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87ef88d2-faa1-41c0-9a41-177dec2356d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook serves as a sandbox to provide exploratory data analysis in preparation for ERD & E2E workflow specifications.\n",
    "It will:\n",
    "1. Create schema & volume if needed.\n",
    "2. Fetch data from [Kaggle competition](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview).\n",
    "3. Create respective tables per csv file.\n",
    "\n",
    "... (wip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd588b5c-dba1-41e5-896f-ca05a842e98a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- creates cscie103_catalog.final_project schema and data volume (if not exist)\n",
    "CREATE SCHEMA IF NOT EXISTS cscie103_catalog.final_project;\n",
    "CREATE VOLUME IF NOT EXISTS cscie103_catalog.final_project.data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c90597be-fc72-4f28-862f-e0d19632c61b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ebde8b8-e1db-4d04-95fa-e292533a6cf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%py\n",
    "# all imports here\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5f8062e-11e3-43ca-93fe-f334f95cb4ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%py #this is the old code with the username as a secret key setup\n",
    "# Kaggle authentication set up\n",
    "kaggle_token = dbutils.secrets.get(scope=\"e-103-finalproject-credentials\", key=\"kaggle-api-token\")\n",
    "kaggle_username = dbutils.secrets.get(scope=\"e-103-finalproject-credentials\", key=\"kaggle-username\")\n",
    "\n",
    "kaggle_config = { \"username\": kaggle_username, \"key\": kaggle_token }\n",
    "kaggle_config_dir = \"/tmp/kaggle_config\"\n",
    "kaggle_file_path = Path(kaggle_config_dir) / \"kaggle.json\"\n",
    "\n",
    "os.makedirs(kaggle_config_dir, exist_ok=True)\n",
    "with open(kaggle_file_path, \"w\") as f:\n",
    "    json.dump(kaggle_config, f)\n",
    "os.chmod(kaggle_file_path, 0o600)\n",
    "\n",
    "os.environ[\"KAGGLE_CONFIG_DIR\"] = kaggle_config_dir\n",
    "os.environ[\"KAGGLE_USERNAME\"] = kaggle_username\n",
    "os.environ[\"KAGGLE_KEY\"] = kaggle_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6873b422-6b6f-43b6-a8fa-e5b43cf83049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%py\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- USE YOUR VALUES HERE ----\n",
    "kaggle_token = \"KGAT_835aa679c6e8303c990aa2b1873a0c10\"\n",
    "kaggle_username = \"kevinalviar\"\n",
    "# --------------------------------\n",
    "\n",
    "# Build kaggle.json config\n",
    "kaggle_config = { \n",
    "    \"username\": kaggle_username, \n",
    "    \"key\": kaggle_token \n",
    "}\n",
    "\n",
    "kaggle_config_dir = \"/tmp/kaggle_config\"\n",
    "kaggle_file_path = Path(kaggle_config_dir) / \"kaggle.json\"\n",
    "\n",
    "# Create the directory and save the kaggle.json file\n",
    "os.makedirs(kaggle_config_dir, exist_ok=True)\n",
    "with open(kaggle_file_path, \"w\") as f:\n",
    "    json.dump(kaggle_config, f)\n",
    "\n",
    "# Secure file permissions\n",
    "os.chmod(kaggle_file_path, 0o600)\n",
    "\n",
    "# Export environment variables for kaggle CLI\n",
    "os.environ[\"KAGGLE_CONFIG_DIR\"] = kaggle_config_dir\n",
    "os.environ[\"KAGGLE_USERNAME\"] = kaggle_username\n",
    "os.environ[\"KAGGLE_KEY\"] = kaggle_token\n",
    "\n",
    "print(\"Kaggle credentials configured successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c567f915-2949-499b-ae3b-ca5bb3b1c301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# only after the Kaggle auth setup\n",
    "import kaggle as kgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7df8bf4a-9b18-4022-acc3-6baf74eb4bf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%py\n",
    "COMPETITION_NAME = \"store-sales-time-series-forecasting\"\n",
    "VOLUME_ROOT_PATH = \"/Volumes/cscie103_catalog/final_project/data\"\n",
    "VOLUME_TARGET_DIR = f\"{VOLUME_ROOT_PATH}/raw\"\n",
    "DOWNLOAD_PATH = VOLUME_TARGET_DIR\n",
    "\n",
    "# download data only if it does not exist in the VOLUME_TARGET_DIR, marker is train.csv file\n",
    "if not os.path.exists(f\"{VOLUME_TARGET_DIR}/train.csv\"):\n",
    "    print(\"Downloading data...\")\n",
    "\n",
    "    os.makedirs(VOLUME_TARGET_DIR, exist_ok=True)\n",
    "    os.makedirs(DOWNLOAD_PATH, exist_ok=True)\n",
    "\n",
    "    # kaggle competitions download -c store-sales-time-series-forecasting -p <path>\n",
    "    command = [\n",
    "        \"kaggle\", \"competitions\", \"download\", \n",
    "        \"-c\", COMPETITION_NAME, \n",
    "        \"-p\", DOWNLOAD_PATH\n",
    "    ]\n",
    "\n",
    "    # --- Download data ---\n",
    "    try:\n",
    "        result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(\"Download successful.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during download: {e}\")\n",
    "        print(f\"Stdout: {e.stdout}\")\n",
    "        print(f\"Stderr: {e.stderr}\")\n",
    "        raise\n",
    "\n",
    "    # --- Unzip ---\n",
    "    zip_file_name = f\"{DOWNLOAD_PATH}/{COMPETITION_NAME}.zip\"\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
    "            zip_ref.extractall(DOWNLOAD_PATH)\n",
    "            print(f\"Extracted files from {zip_file_name}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: ZIP file not found at {zip_file_name}. Download may have failed.\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"Skipped downloading data because file-marker exists already (train.csv).\")\n",
    "\n",
    "file_path = f\"{VOLUME_TARGET_DIR}/train.csv\"\n",
    "try:\n",
    "    df = (spark.read\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"inferSchema\", \"true\")\n",
    "      .csv(file_path)\n",
    "    )\n",
    "    print(\"\\nVerification: Data loaded into Spark DataFrame.\")\n",
    "    df.printSchema()\n",
    "    # df.display()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading file from Volume: {e}. Check file path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71bf8104-feaf-490d-aaea-496abd169f30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python # manually putting in kaggle key\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "# ---- USE YOUR REAL kaggle.json VALUES HERE ----\n",
    "kaggle_username = \"kevinalviar\"             # from kaggle.json[\"username\"]\n",
    "kaggle_token    = \"15446cb3990c4f8071488e98244d9010\"    # from kaggle.json[\"key\"]\n",
    "# -----------------------------------------------\n",
    "\n",
    "COMPETITION_NAME   = \"store-sales-time-series-forecasting\"\n",
    "VOLUME_ROOT_PATH   = \"/Volumes/cscie103_catalog/final_project/data\"\n",
    "VOLUME_TARGET_DIR  = f\"{VOLUME_ROOT_PATH}/raw\"\n",
    "DOWNLOAD_PATH      = VOLUME_TARGET_DIR\n",
    "ZIP_PATH           = f\"{DOWNLOAD_PATH}/{COMPETITION_NAME}.zip\"\n",
    "\n",
    "os.makedirs(DOWNLOAD_PATH, exist_ok=True)\n",
    "\n",
    "# Only download if train.csv not present\n",
    "if not os.path.exists(f\"{VOLUME_TARGET_DIR}/train.csv\"):\n",
    "    print(\"Downloading data directly from Kaggle API...\")\n",
    "\n",
    "    url = f\"https://www.kaggle.com/api/v1/competitions/data/download-all/{COMPETITION_NAME}\"\n",
    "\n",
    "    resp = requests.get(\n",
    "        url,\n",
    "        auth=HTTPBasicAuth(kaggle_username, kaggle_token),\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download data: HTTP {resp.status_code}\\n{resp.text}\")\n",
    "\n",
    "    # Write ZIP\n",
    "    with open(ZIP_PATH, \"wb\") as f:\n",
    "        for chunk in resp.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "    print(f\"Download complete: {ZIP_PATH}\")\n",
    "\n",
    "    # Extract ZIP\n",
    "    with zipfile.ZipFile(ZIP_PATH, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(DOWNLOAD_PATH)\n",
    "\n",
    "    print(f\"Extracted files to: {DOWNLOAD_PATH}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipped downloading because train.csv already exists.\")\n",
    "\n",
    "# Verify train.csv\n",
    "file_path = f\"{VOLUME_TARGET_DIR}/train.csv\"\n",
    "try:\n",
    "    df = (\n",
    "        spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .csv(file_path)\n",
    "    )\n",
    "    print(\"\\nVerification: Data loaded into Spark DataFrame.\")\n",
    "    df.printSchema()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading file from Volume: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "741546d6-a1fd-41de-b681-bd579238b3e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filenames = {\n",
    "    'holidays_events': 'holidays_events.csv',\n",
    "    'oil': 'oil.csv',\n",
    "    'sample_submission': 'sample_submission.csv',\n",
    "    'stores': 'stores.csv',\n",
    "    'test': 'test.csv',\n",
    "    'train': 'train.csv',\n",
    "    'transactions': 'transactions.csv'\n",
    "}\n",
    "\n",
    "holidays_events_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('holidays_events')}\", header=True, inferSchema=True)\n",
    "oil_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('oil')}\", header=True, inferSchema=True)\n",
    "stores_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('stores')}\", header=True, inferSchema=True)\n",
    "transactions_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('transactions')}\", header=True, inferSchema=True)\n",
    "train_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames.get('train')}\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90d201fc-16ed-46eb-839c-badfa0b1d58c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "/Volumes/cscie103_catalog/final_project/data/raw/\n",
    "    train.csv\n",
    "    test.csv\n",
    "    stores.csv\n",
    "    oil.csv\n",
    "    holidays_events.csv\n",
    "    transactions.csv\n",
    "    sample_submission.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22b50034-9c9e-413c-b41c-976e412ccde8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading in CSVs\n",
    "VOLUME_ROOT_PATH  = \"/Volumes/cscie103_catalog/final_project/data\"\n",
    "VOLUME_TARGET_DIR = f\"{VOLUME_ROOT_PATH}/raw\"\n",
    "\n",
    "filenames = {\n",
    "    \"holidays_events\": \"holidays_events.csv\",\n",
    "    \"oil\": \"oil.csv\",\n",
    "    \"sample_submission\": \"sample_submission.csv\",\n",
    "    \"stores\": \"stores.csv\",\n",
    "    \"test\": \"test.csv\",\n",
    "    \"train\": \"train.csv\",\n",
    "    \"transactions\": \"transactions.csv\",\n",
    "}\n",
    "\n",
    "holidays_events_df = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames['holidays_events']}\", header=True, inferSchema=True)\n",
    "oil_df             = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames['oil']}\",              header=True, inferSchema=True)\n",
    "stores_df          = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames['stores']}\",           header=True, inferSchema=True)\n",
    "transactions_df    = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames['transactions']}\",     header=True, inferSchema=True)\n",
    "train_df           = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames['train']}\",            header=True, inferSchema=True)\n",
    "test_df            = spark.read.csv(f\"{VOLUME_TARGET_DIR}/{filenames['test']}\",             header=True, inferSchema=True)\n",
    "\n",
    "print(\"Loaded DataFrames.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc64103b-4d81-40e9-890b-3aaaf4f78463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We are now writing Bronze Delta Tables one per CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a3b500-0762-438f-ac72-aa226d3c4501",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Use your UC catalog & schema\n",
    "catalog = \"cscie103_catalog\"\n",
    "schema = \"final_project\"\n",
    "\n",
    "spark.sql(f\"USE {catalog}.{schema}\")\n",
    "\n",
    "print(f\"Writing Bronze tables into {catalog}.{schema} ...\")\n",
    "\n",
    "# ---- WRITE MANAGED DELTA TABLES (UC FRIENDLY) ----\n",
    "train_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_train\")\n",
    "test_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_test\")\n",
    "stores_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_stores\")\n",
    "oil_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_oil\")\n",
    "holidays_events_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_holidays_events\")\n",
    "transactions_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_transactions\")\n",
    "\n",
    "print(\"✅ Bronze Delta tables created successfully as managed UC tables!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23a442dd-7862-45be-9a09-3b8a28b00d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Silver Format Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1360c4be-535d-4ab7-a64a-7cdabcfb7937",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Make sure we're in the right catalog & schema\n",
    "spark.sql(\"USE cscie103_catalog.final_project\")\n",
    "\n",
    "# Path for checkpoint inside your UC Volume\n",
    "checkpoint_path = \"/Volumes/cscie103_catalog/final_project/data/checkpoints/silver_train\"\n",
    "\n",
    "# Read from Bronze as a streaming source\n",
    "bronze_train_stream = (\n",
    "    spark.readStream\n",
    "         .table(\"bronze_train\")   # managed UC Delta table\n",
    ")\n",
    "\n",
    "# Apply cleaning / typing\n",
    "silver_train_stream = (\n",
    "    bronze_train_stream\n",
    "    .withColumn(\"date\", F.to_date(\"date\"))\n",
    "    .withColumn(\"store_nbr\", F.col(\"store_nbr\").cast(\"int\"))\n",
    "    .withColumn(\"onpromotion\", F.col(\"onpromotion\").cast(\"int\"))\n",
    "    .withColumn(\"sales\", F.col(\"sales\").cast(\"double\"))\n",
    "    .withColumn(\"family\", F.col(\"family\").cast(\"string\")) \n",
    ")\n",
    "\n",
    "# Write as managed Delta table using trigger=once\n",
    "query = (\n",
    "    silver_train_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)  # ✅ now in a UC Volume, not public DBFS root\n",
    "    .trigger(once=True)\n",
    "    .toTable(\"silver_train\")   # creates/updates UC managed table cscie103_catalog.final_project.silver_train\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "print(\"✅ Silver table 'silver_train' created via streaming with trigger once.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49db91c7-af1e-4b4b-8f3d-91ae33468099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.sql(\"USE cscie103_catalog.final_project\")\n",
    "\n",
    "# ---- Silver STORES ----\n",
    "silver_stores = (\n",
    "    spark.table(\"bronze_stores\")\n",
    "    .withColumn(\"store_nbr\", F.col(\"store_nbr\").cast(\"int\"))\n",
    "    .withColumn(\"cluster\", F.col(\"cluster\").cast(\"int\"))\n",
    ")\n",
    "\n",
    "silver_stores.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_stores\")\n",
    "\n",
    "\n",
    "# ---- Silver OIL ----\n",
    "silver_oil = (\n",
    "    spark.table(\"bronze_oil\")\n",
    "    .withColumn(\"date\", F.to_date(\"date\"))\n",
    "    .withColumn(\"dcoilwtico\", F.col(\"dcoilwtico\").cast(\"double\"))\n",
    ")\n",
    "\n",
    "silver_oil.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_oil\")\n",
    "\n",
    "\n",
    "# ---- Silver HOLIDAYS_EVENTS ----\n",
    "silver_holidays = (\n",
    "    spark.table(\"bronze_holidays_events\")\n",
    "    .withColumn(\"date\", F.to_date(\"date\"))\n",
    "    .withColumn(\"is_holiday\", (F.col(\"type\") != \"Work Day\").cast(\"int\"))\n",
    ")\n",
    "\n",
    "silver_holidays.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_holidays_events\")\n",
    "\n",
    "\n",
    "# ---- Silver TRANSACTIONS ----\n",
    "silver_transactions = (\n",
    "    spark.table(\"bronze_transactions\")\n",
    "    .withColumn(\"date\", F.to_date(\"date\"))\n",
    "    .withColumn(\"store_nbr\", F.col(\"store_nbr\").cast(\"int\"))\n",
    "    .withColumn(\"transactions\", F.col(\"transactions\").cast(\"int\"))\n",
    ")\n",
    "\n",
    "silver_transactions.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_transactions\")\n",
    "\n",
    "\n",
    "print(\"Silver tables created: silver_stores, silver_oil, silver_holidays_events, silver_transactions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e3084f9-8656-4e5f-9864-59cedec913a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/Volumes/cscie103_catalog/final_project/data/checkpoints\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6076f328-dd43-4784-80f1-cf519892221b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"tableName\":195},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764573623648}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SHOW TABLES IN cscie103_catalog.final_project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b10f7f4-22cf-471e-8c94-83c8ff269595",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Building Out Gold Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e36281a-40bc-4767-a7fe-e45bee6263f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Use the right catalog & schema\n",
    "spark.sql(\"USE cscie103_catalog.final_project\")\n",
    "\n",
    "# Load Silver tables\n",
    "silver_train  = spark.table(\"silver_train\")\n",
    "silver_stores = spark.table(\"silver_stores\")\n",
    "silver_oil    = spark.table(\"silver_oil\")\n",
    "silver_hol    = spark.table(\"silver_holidays_events\")\n",
    "silver_tx     = spark.table(\"silver_transactions\")\n",
    "\n",
    "# Enriched base fact: one row per date-store-family\n",
    "base_fact = (\n",
    "    silver_train.alias(\"t\")\n",
    "    .join(silver_stores.alias(\"s\"), \"store_nbr\", \"left\")\n",
    "    .join(silver_tx.alias(\"x\"), [\"date\", \"store_nbr\"], \"left\")\n",
    "    .join(silver_oil.alias(\"o\"), \"date\", \"left\")\n",
    "    .join(silver_hol.alias(\"h\"), \"date\", \"left\")\n",
    "    .select(\n",
    "        F.col(\"t.date\").alias(\"date\"),\n",
    "        F.col(\"t.store_nbr\").alias(\"store_nbr\"),\n",
    "        F.col(\"t.family\").alias(\"family\"),\n",
    "        F.col(\"t.sales\").alias(\"sales\"),\n",
    "        F.col(\"t.onpromotion\").alias(\"onpromotion\"),\n",
    "        F.col(\"x.transactions\").alias(\"transactions\"),\n",
    "        F.col(\"o.dcoilwtico\").alias(\"dcoilwtico\"),\n",
    "        F.col(\"s.city\").alias(\"city\"),\n",
    "        F.col(\"s.state\").alias(\"state\"),\n",
    "        F.col(\"s.type\").alias(\"store_type\"),\n",
    "        F.col(\"s.cluster\").alias(\"cluster\"),\n",
    "        F.col(\"h.is_holiday\").alias(\"is_holiday\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Initial write of Gold fact table as managed UC table\n",
    "base_fact.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_daily_store_family\")\n",
    "\n",
    "print(\"✅ Gold table 'gold_daily_store_family' created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c80c611-b309-40b0-afaa-b77cf06e5294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"cscie103_catalog.final_project.gold_daily_store_family\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6843e54-7003-4ba6-9bbc-f2f605dd5b21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM cscie103_catalog.final_project.gold_daily_store_family\n",
    "WHERE sales = 1\n",
    "  AND is_holiday IS NOT NULL LIMIT 50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff2423ea-8024-4a23-b85a-5c69703813f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4938962754428469,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "KA_sandbox-exploratory-data-analysis (1)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
